{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import matplotlib.cm as cm\n",
    "import sklearn\n",
    "from sklearn import svm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "data=pd.read_csv(\"/Users/Cristy/Downloads/task2/X_train.csv\")\n",
    "labels=pd.read_csv(\"/Users/Cristy/Downloads/task2/y_train.csv\")\n",
    "data['labels']=pd.Series(labels['y'])\n",
    "data.drop(columns='id',inplace=True)\n",
    "\n",
    "std_data=data.loc[:,~data.columns.isin(['labels'])].apply(lambda x: (x-x.mean())/x.std())\n",
    "std_data['labels']=data['labels']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4800, 1001) (4800, 964)\n"
     ]
    }
   ],
   "source": [
    "## correlated features\n",
    "corr_feats=set()\n",
    "corr_matrix=std_data.drop(columns='labels').corr()\n",
    "\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(corr_matrix.iloc[i,j])>0.8:\n",
    "            correlated=corr_matrix.columns[i]\n",
    "            corr_feats.add(correlated)\n",
    "        \n",
    "data_uncorr=std_data.drop(columns=corr_feats)\n",
    "print(std_data.shape,data_uncorr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x142',\n",
       " 'x144',\n",
       " 'x196',\n",
       " 'x222',\n",
       " 'x233',\n",
       " 'x279',\n",
       " 'x311',\n",
       " 'x345',\n",
       " 'x362',\n",
       " 'x371',\n",
       " 'x373',\n",
       " 'x399',\n",
       " 'x419',\n",
       " 'x498',\n",
       " 'x504',\n",
       " 'x507',\n",
       " 'x532',\n",
       " 'x640',\n",
       " 'x686',\n",
       " 'x705',\n",
       " 'x746',\n",
       " 'x794',\n",
       " 'x807',\n",
       " 'x812',\n",
       " 'x821',\n",
       " 'x840',\n",
       " 'x875',\n",
       " 'x878',\n",
       " 'x884',\n",
       " 'x898',\n",
       " 'x904',\n",
       " 'x907',\n",
       " 'x937',\n",
       " 'x940',\n",
       " 'x972',\n",
       " 'x983',\n",
       " 'x995'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1\n",
    "\n",
    "PCA+SVM rbf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n",
      " Train score: 0.8671920014514996\n",
      "Test score: 0.6921497576436747\n",
      "Fold: 2\n",
      " Train score: 0.8649193195970012\n",
      "Test score: 0.6838448869676462\n",
      "Fold: 3\n",
      " Train score: 0.8639719804671445\n",
      "Test score: 0.6898984235652083\n",
      "Fold: 4\n",
      " Train score: 0.871442928058201\n",
      "Test score: 0.7215141612200435\n",
      "Fold: 5\n",
      " Train score: 0.8634014909783652\n",
      "Test score: 0.7038768010143818\n",
      "Fold: 6\n",
      " Train score: 0.8698656678711992\n",
      "Test score: 0.6540622512162239\n",
      "Fold: 7\n",
      " Train score: 0.8648459314231053\n",
      "Test score: 0.690289340718874\n",
      "Fold: 8\n",
      " Train score: 0.8635577746275699\n",
      "Test score: 0.6417681286179241\n",
      "Fold: 9\n",
      " Train score: 0.8631216744214445\n",
      "Test score: 0.6856617042952445\n",
      "Fold: 10\n",
      " Train score: 0.8636148184257699\n",
      "Test score: 0.7227357223113687\n",
      "Mean train score: 0.8655933587321301\n",
      " Mean test score:0.688580117757059\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components = 0.8)\n",
    "## Cross-validation\n",
    "data=data_uncorr.drop(columns='labels')\n",
    "labels=data_uncorr.labels\n",
    "kf=KFold(10,shuffle=True)\n",
    "folds=kf.split(data)\n",
    "i=0\n",
    "test_scores=[]\n",
    "train_scores=[]\n",
    "for train_index, test_index in folds:\n",
    "    X_train=data.values[train_index]\n",
    "    X_test=data.values[test_index]\n",
    "    y_train=labels.ravel()[train_index]\n",
    "    y_test=labels.ravel()[test_index]\n",
    "    pca.fit(pd.DataFrame(X_train))\n",
    "    X_trainpca=pca.transform(pd.DataFrame(X_train))\n",
    "    svm_mod=svm.SVC(kernel='rbf',gamma='scale',class_weight='balanced').fit(X_trainpca,y_train)\n",
    "    train_pred=svm_mod.predict(X_trainpca)\n",
    "    train_score=balanced_accuracy_score(y_train,train_pred)\n",
    "    train_scores.append(train_score)\n",
    "    i+=1\n",
    "    print(\"Fold: {}\\n Train score: {}\".format(i,train_score))\n",
    "    X_testpca=pca.transform(X_test)\n",
    "    test_pred=svm_mod.predict(X_testpca)\n",
    "    test_score=balanced_accuracy_score(y_test,test_pred)\n",
    "    print(\"Test score: {}\".format(test_score))\n",
    "    test_scores.append(test_score)\n",
    "    \n",
    "print(\"Mean train score: {}\\n Mean test score:{}\".format(np.mean(train_scores),np.mean(test_scores)))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Same model, stratified kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n",
      " Train score: 0.8477366255144032\n",
      "Test score: 0.6935185185185185\n",
      "Fold: 2\n",
      " Train score: 0.8494855967078189\n",
      "Test score: 0.6712962962962963\n",
      "Fold: 3\n",
      " Train score: 0.8481481481481481\n",
      "Test score: 0.7018518518518518\n",
      "Fold: 4\n",
      " Train score: 0.8487654320987654\n",
      "Test score: 0.7046296296296296\n",
      "Fold: 5\n",
      " Train score: 0.8495884773662551\n",
      "Test score: 0.7037037037037037\n",
      "Fold: 6\n",
      " Train score: 0.8473251028806584\n",
      "Test score: 0.65\n",
      "Fold: 7\n",
      " Train score: 0.8476337448559671\n",
      "Test score: 0.6462962962962963\n",
      "Fold: 8\n",
      " Train score: 0.8437242798353909\n",
      "Test score: 0.7055555555555556\n",
      "Fold: 9\n",
      " Train score: 0.8482510288065844\n",
      "Test score: 0.725\n",
      "Fold: 10\n",
      " Train score: 0.8521604938271605\n",
      "Test score: 0.6805555555555555\n",
      "Mean train score: 0.8521604938271605\n",
      " Mean test score:0.6805555555555555\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components = 0.8)\n",
    "## Cross-validation\n",
    "data=data_uncorr.drop(columns='labels')\n",
    "labels=data_uncorr.labels\n",
    "kf=StratifiedKFold(10,shuffle=True)\n",
    "folds=kf.split(data,labels)\n",
    "i=0\n",
    "test_scores=[]\n",
    "train_scores=[]\n",
    "for train_index, test_index in folds:\n",
    "    X_train=data.values[train_index]\n",
    "    X_test=data.values[test_index]\n",
    "    y_train=labels.ravel()[train_index]\n",
    "    y_test=labels.ravel()[test_index]\n",
    "    pca.fit(pd.DataFrame(X_train))\n",
    "    X_trainpca=pca.transform(pd.DataFrame(X_train))\n",
    "    svm_mod=svm.SVC(kernel='rbf',gamma='scale',class_weight='balanced').fit(X_trainpca,y_train)\n",
    "    train_pred=svm_mod.predict(X_trainpca)\n",
    "    train_score=balanced_accuracy_score(y_train,train_pred)\n",
    "    train_scores.append(train_score)\n",
    "    i+=1\n",
    "    print(\"Fold: {}\\n Train score: {}\".format(i,train_score))\n",
    "    X_testpca=pca.transform(X_test)\n",
    "    test_pred=svm_mod.predict(X_testpca)\n",
    "    test_score=balanced_accuracy_score(y_test,test_pred)\n",
    "    print(\"Test score: {}\".format(test_score))\n",
    "    test_scores.append(test_score)\n",
    "    \n",
    "print(\"Mean train score: {}\\n Mean test score:{}\".format(np.mean(train_scores),np.mean(test_scores)))\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "features with corr < 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4800, 1001) (4800, 771)\n"
     ]
    }
   ],
   "source": [
    "## correlated features\n",
    "corr_feats=set()\n",
    "corr_matrix=std_data.drop(columns='labels').corr()\n",
    "\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(corr_matrix.iloc[i,j])>0.7:\n",
    "            correlated=corr_matrix.columns[i]\n",
    "            corr_feats.add(correlated)\n",
    "        \n",
    "data_uncorr=std_data.drop(columns=corr_feats)\n",
    "print(std_data.shape,data_uncorr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n",
      " Train score: 0.8660552735993153\n",
      "Test score: 0.6930640770059547\n",
      "Fold: 2\n",
      " Train score: 0.8647786215108471\n",
      "Test score: 0.6443490352762109\n",
      "Fold: 3\n",
      " Train score: 0.8625996937341499\n",
      "Test score: 0.7264541602178146\n",
      "Fold: 4\n",
      " Train score: 0.8652879121749001\n",
      "Test score: 0.7238417828122975\n",
      "Fold: 5\n",
      " Train score: 0.8636138558401111\n",
      "Test score: 0.7164327094474153\n",
      "Fold: 6\n",
      " Train score: 0.8675646388347252\n",
      "Test score: 0.6626750823720992\n",
      "Fold: 7\n",
      " Train score: 0.8654240144153462\n",
      "Test score: 0.6903782280937453\n",
      "Fold: 8\n",
      " Train score: 0.8612530242817075\n",
      "Test score: 0.6810393262181024\n",
      "Fold: 9\n",
      " Train score: 0.8782336704985872\n",
      "Test score: 0.6448607761476222\n",
      "Fold: 10\n",
      " Train score: 0.8619352532878849\n",
      "Test score: 0.7441771475197658\n",
      "Mean train score: 0.8619352532878849\n",
      " Mean test score:0.7441771475197658\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components = 0.8)\n",
    "## Cross-validation\n",
    "data=data_uncorr.drop(columns='labels')\n",
    "labels=data_uncorr.labels\n",
    "kf=KFold(10,shuffle=True)\n",
    "folds=kf.split(data)\n",
    "i=0\n",
    "test_scores=[]\n",
    "train_scores=[]\n",
    "for train_index, test_index in folds:\n",
    "    X_train=data.values[train_index]\n",
    "    X_test=data.values[test_index]\n",
    "    y_train=labels.ravel()[train_index]\n",
    "    y_test=labels.ravel()[test_index]\n",
    "    pca.fit(pd.DataFrame(X_train))\n",
    "    X_trainpca=pca.transform(pd.DataFrame(X_train))\n",
    "    svm_mod=svm.SVC(kernel='rbf',gamma='scale',class_weight='balanced').fit(X_trainpca,y_train)\n",
    "    train_pred=svm_mod.predict(X_trainpca)\n",
    "    train_score=balanced_accuracy_score(y_train,train_pred)\n",
    "    train_scores.append(train_score)\n",
    "    i+=1\n",
    "    print(\"Fold: {}\\n Train score: {}\".format(i,train_score))\n",
    "    X_testpca=pca.transform(X_test)\n",
    "    test_pred=svm_mod.predict(X_testpca)\n",
    "    test_score=balanced_accuracy_score(y_test,test_pred)\n",
    "    print(\"Test score: {}\".format(test_score))\n",
    "    test_scores.append(test_score)\n",
    "    \n",
    "print(\"Mean train score: {}\\n Mean test score:{}\".format(np.mean(train_scores),np.mean(test_scores)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      " Train score: 1.0\n",
      "Test score:0.5887438898868577\n",
      "Fold 2\n",
      " Train score: 1.0\n",
      "Test score:0.5711383191562998\n",
      "Fold 3\n",
      " Train score: 1.0\n",
      "Test score:0.5751305562086334\n",
      "Fold 4\n",
      " Train score: 1.0\n",
      "Test score:0.5928991581839315\n",
      "Fold 5\n",
      " Train score: 1.0\n",
      "Test score:0.5880361138047522\n",
      "Fold 6\n",
      " Train score: 1.0\n",
      "Test score:0.5702818732269882\n",
      "Fold 7\n",
      " Train score: 1.0\n",
      "Test score:0.5713099415204679\n",
      "Fold 8\n",
      " Train score: 1.0\n",
      "Test score:0.5937486480640277\n",
      "Fold 9\n",
      " Train score: 1.0\n",
      "Test score:0.5376720980169256\n",
      "Fold 10\n",
      " Train score: 1.0\n",
      "Test score:0.6104272886640706\n",
      "Mean train score:1.0\n",
      " Mean test score: 0.5799387886732955\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "data=data_uncorr.drop(columns='labels')\n",
    "labels=data_uncorr['labels']\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "#X_lda = lda.fit_transform(data, labels)\n",
    "\n",
    "\n",
    "kf=KFold(n_splits=10,shuffle=True)\n",
    "folds=kf.split(data)\n",
    "train_scores=[]\n",
    "test_scores=[]\n",
    "i=0\n",
    "for train_index, test_index in folds:\n",
    "    X_train=data.values[train_index]\n",
    "    X_test=data.values[test_index]\n",
    "    y_train=labels.ravel()[train_index]\n",
    "    y_test=labels.ravel()[test_index]\n",
    "    X_lda=lda.fit_transform(X_train,y_train)\n",
    "    dt=DecisionTreeClassifier(class_weight='balanced')\n",
    "    dt.fit(X_lda,y_train)\n",
    "    # Training \n",
    "    train_preds=dt.predict(X_lda)\n",
    "    trainscore=balanced_accuracy_score(y_train,train_preds)\n",
    "    i+=1\n",
    "    print(\"Fold {}\\n Train score: {}\".format(i,trainscore))\n",
    "    train_scores.append(trainscore)\n",
    "    # Testing\n",
    "    X_testlda=lda.transform(X_test)\n",
    "    test_preds=dt.predict(X_testlda)\n",
    "    testscore=balanced_accuracy_score(y_test,test_preds)\n",
    "    print(\"Test score:{}\".format(testscore))\n",
    "    test_scores.append(testscore)\n",
    "    \n",
    "print(\"Mean train score:{}\\n Mean test score: {}\".format(np.mean(train_scores),np.mean(test_scores)))    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stratified kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      " Train score: 1.0\n",
      "Test score:0.6111111111111112\n",
      "Fold 2\n",
      " Train score: 1.0\n",
      "Test score:0.5583333333333332\n",
      "Fold 3\n",
      " Train score: 1.0\n",
      "Test score:0.5861111111111111\n",
      "Fold 4\n",
      " Train score: 1.0\n",
      "Test score:0.6185185185185186\n",
      "Fold 5\n",
      " Train score: 1.0\n",
      "Test score:0.5648148148148149\n",
      "Fold 6\n",
      " Train score: 1.0\n",
      "Test score:0.5555555555555556\n",
      "Fold 7\n",
      " Train score: 1.0\n",
      "Test score:0.612037037037037\n",
      "Fold 8\n",
      " Train score: 1.0\n",
      "Test score:0.5583333333333332\n",
      "Fold 9\n",
      " Train score: 1.0\n",
      "Test score:0.5851851851851851\n",
      "Fold 10\n",
      " Train score: 1.0\n",
      "Test score:0.5740740740740741\n",
      "Mean train score:1.0\n",
      " Mean test score: 0.5824074074074075\n"
     ]
    }
   ],
   "source": [
    "data=data_uncorr.drop(columns='labels')\n",
    "labels=data_uncorr['labels']\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "#X_lda = lda.fit_transform(data, labels)\n",
    "\n",
    "\n",
    "kf=StratifiedKFold(n_splits=10,shuffle=True)\n",
    "folds=kf.split(data,labels)\n",
    "train_scores=[]\n",
    "test_scores=[]\n",
    "i=0\n",
    "for train_index, test_index in folds:\n",
    "    X_train=data.values[train_index]\n",
    "    X_test=data.values[test_index]\n",
    "    y_train=labels.ravel()[train_index]\n",
    "    y_test=labels.ravel()[test_index]\n",
    "    X_lda=lda.fit_transform(X_train,y_train)\n",
    "    dt=DecisionTreeClassifier(class_weight='balanced')\n",
    "    dt.fit(X_lda,y_train)\n",
    "    # Training \n",
    "    train_preds=dt.predict(X_lda)\n",
    "    trainscore=balanced_accuracy_score(y_train,train_preds)\n",
    "    i+=1\n",
    "    print(\"Fold {}\\n Train score: {}\".format(i,trainscore))\n",
    "    train_scores.append(trainscore)\n",
    "    # Testing\n",
    "    X_testlda=lda.transform(X_test)\n",
    "    test_preds=dt.predict(X_testlda)\n",
    "    testscore=balanced_accuracy_score(y_test,test_preds)\n",
    "    print(\"Test score:{}\".format(testscore))\n",
    "    test_scores.append(testscore)\n",
    "    \n",
    "print(\"Mean train score:{}\\n Mean test score: {}\".format(np.mean(train_scores),np.mean(test_scores)))    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      " Train score:0.8432457920926492\n",
      "Test score:0.6383856117353308\n",
      "\n",
      "Fold 2\n",
      " Train score:0.8420819618282209\n",
      "Test score:0.6729478692270372\n",
      "\n",
      "Fold 3\n",
      " Train score:0.8382979045966014\n",
      "Test score:0.6443024855371511\n",
      "\n",
      "Fold 4\n",
      " Train score:0.8413552003262111\n",
      "Test score:0.6266109908237962\n",
      "\n",
      "Fold 5\n",
      " Train score:0.8383946919429257\n",
      "Test score:0.6500240760804141\n",
      "\n",
      "Fold 6\n",
      " Train score:0.8306853748485987\n",
      "Test score:0.688917735805514\n",
      "\n",
      "Fold 7\n",
      " Train score:0.8377817522520324\n",
      "Test score:0.6269097010784991\n",
      "\n",
      "Fold 8\n",
      " Train score:0.8389888621773732\n",
      "Test score:0.6402372418501451\n",
      "\n",
      "Fold 9\n",
      " Train score:0.8423291722296096\n",
      "Test score:0.633603877856154\n",
      "\n",
      "Fold 10\n",
      " Train score:0.8450691250606703\n",
      "Test score:0.6470685408782363\n",
      "\n",
      "Mean train score: 0.8398229837354891\n",
      " Mean test score:0.6469008130872278\n"
     ]
    }
   ],
   "source": [
    "data=data_uncorr.drop(columns='labels')\n",
    "labels=data_uncorr['labels']\n",
    "#X_train, X_test, y_train, y_test = train_test_split(data, labels, random_state=1)\n",
    "#lda.fit(data,labels)\n",
    "#X_train_lda=lda.transform(X_train)\n",
    "#svm1=svm.SVC(kernel='rbf',gamma='scale',class_weight='balanced')\n",
    "#svm1.fit(X_train_lda,y_train)\n",
    "\n",
    "#train_pred=svm1.predict(X_train_lda)\n",
    "#print('Train score:', balanced_accuracy_score(y_train,train_pred))\n",
    "\n",
    "#X_test=lda.transform(X_test)\n",
    "#test_pred=svm1.predict(X_test)\n",
    "#print('Test score:', balanced_accuracy_score(y_test,test_pred))\n",
    "\n",
    "\n",
    "\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "# Cross validation\n",
    "kf=KFold(n_splits=10)\n",
    "folds=kf.split(data)\n",
    "train_scores=[]\n",
    "test_scores=[]\n",
    "i=0\n",
    "for train_index, test_index in folds:\n",
    "    x_train=data.values[train_index]\n",
    "    x_test=data.values[test_index]\n",
    "    y_train=labels.ravel()[train_index]\n",
    "    y_test=labels.ravel()[test_index]\n",
    "    x_trainlda=lda.fit_transform(x_train,y_train)\n",
    "    svm1=svm.SVC(kernel='rbf',gamma='scale',class_weight='balanced').fit(x_trainlda,y_train)\n",
    "    \n",
    "    #training scores\n",
    "    train_preds=svm1.predict(x_trainlda)\n",
    "    trainscore=balanced_accuracy_score(y_train,train_preds)\n",
    "    train_scores.append(trainscore)\n",
    "    i+=1\n",
    "    print(\"Fold {}\\n Train score:{}\".format(i,trainscore))\n",
    "    \n",
    "    #testing scores\n",
    "    x_testlda=lda.transform(x_test)\n",
    "    test_preds=svm1.predict(x_testlda)\n",
    "    testscore=balanced_accuracy_score(y_test,test_preds)\n",
    "    print(\"Test score:{}\\n\".format(testscore))\n",
    "    test_scores.append(testscore)\n",
    "    \n",
    "print(\"Mean train score: {}\\n Mean test score:{}\".format(np.mean(train_scores),np.mean(test_scores)))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stratified kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      " Train score:0.8450617283950618\n",
      "Test score:0.6333333333333333\n",
      "\n",
      "Fold 2\n",
      " Train score:0.840843621399177\n",
      "Test score:0.6703703703703704\n",
      "\n",
      "Fold 3\n",
      " Train score:0.841358024691358\n",
      "Test score:0.6444444444444444\n",
      "\n",
      "Fold 4\n",
      " Train score:0.8406378600823046\n",
      "Test score:0.6166666666666667\n",
      "\n",
      "Fold 5\n",
      " Train score:0.8393004115226338\n",
      "Test score:0.6518518518518518\n",
      "\n",
      "Fold 6\n",
      " Train score:0.8360082304526749\n",
      "Test score:0.686111111111111\n",
      "\n",
      "Fold 7\n",
      " Train score:0.8440329218106996\n",
      "Test score:0.6388888888888888\n",
      "\n",
      "Fold 8\n",
      " Train score:0.8369341563786009\n",
      "Test score:0.662962962962963\n",
      "\n",
      "Fold 9\n",
      " Train score:0.8436213991769547\n",
      "Test score:0.6398148148148148\n",
      "\n",
      "Fold 10\n",
      " Train score:0.8444444444444444\n",
      "Test score:0.638888888888889\n",
      "\n",
      "Mean train score: 0.8412242798353908\n",
      " Mean test score:0.6483333333333333\n"
     ]
    }
   ],
   "source": [
    "data=data_uncorr.drop(columns='labels')\n",
    "labels=data_uncorr['labels']\n",
    "#X_train, X_test, y_train, y_test = train_test_split(data, labels, random_state=1)\n",
    "#lda.fit(data,labels)\n",
    "#X_train_lda=lda.transform(X_train)\n",
    "#svm1=svm.SVC(kernel='rbf',gamma='scale',class_weight='balanced')\n",
    "#svm1.fit(X_train_lda,y_train)\n",
    "\n",
    "#train_pred=svm1.predict(X_train_lda)\n",
    "#print('Train score:', balanced_accuracy_score(y_train,train_pred))\n",
    "\n",
    "#X_test=lda.transform(X_test)\n",
    "#test_pred=svm1.predict(X_test)\n",
    "#print('Test score:', balanced_accuracy_score(y_test,test_pred))\n",
    "\n",
    "\n",
    "\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "# Cross validation\n",
    "kf=StratifiedKFold(n_splits=10)\n",
    "folds=kf.split(data,labels)\n",
    "train_scores=[]\n",
    "test_scores=[]\n",
    "i=0\n",
    "for train_index, test_index in folds:\n",
    "    x_train=data.values[train_index]\n",
    "    x_test=data.values[test_index]\n",
    "    y_train=labels.ravel()[train_index]\n",
    "    y_test=labels.ravel()[test_index]\n",
    "    x_trainlda=lda.fit_transform(x_train,y_train)\n",
    "    svm1=svm.SVC(kernel='rbf',gamma='scale',class_weight='balanced').fit(x_trainlda,y_train)\n",
    "    \n",
    "    #training scores\n",
    "    train_preds=svm1.predict(x_trainlda)\n",
    "    trainscore=balanced_accuracy_score(y_train,train_preds)\n",
    "    train_scores.append(trainscore)\n",
    "    i+=1\n",
    "    print(\"Fold {}\\n Train score:{}\".format(i,trainscore))\n",
    "    \n",
    "    #testing scores\n",
    "    x_testlda=lda.transform(x_test)\n",
    "    test_preds=svm1.predict(x_testlda)\n",
    "    testscore=balanced_accuracy_score(y_test,test_preds)\n",
    "    print(\"Test score:{}\\n\".format(testscore))\n",
    "    test_scores.append(testscore)\n",
    "    \n",
    "print(\"Mean train score: {}\\n Mean test score:{}\".format(np.mean(train_scores),np.mean(test_scores)))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      " Train score: 0.7318196331608418\n",
      "Test score:0.7222370822776503\n",
      "\n",
      "Fold 2\n",
      " Train score: 0.7286295218939438\n",
      "Test score:0.728895694469465\n",
      "\n",
      "Fold 3\n",
      " Train score: 0.7364045697362073\n",
      "Test score:0.6863555463097798\n",
      "\n",
      "Fold 4\n",
      " Train score: 0.7389980774771464\n",
      "Test score:0.6949065734933484\n",
      "\n",
      "Fold 5\n",
      " Train score: 0.7412499869084002\n",
      "Test score:0.702189829558543\n",
      "\n",
      "Fold 6\n",
      " Train score: 0.7384081869658815\n",
      "Test score:0.7047875343558401\n",
      "\n",
      "Fold 7\n",
      " Train score: 0.7380492236357368\n",
      "Test score:0.6606161585638822\n",
      "\n",
      "Fold 8\n",
      " Train score: 0.7339906520779187\n",
      "Test score:0.6768454754137974\n",
      "\n",
      "Fold 9\n",
      " Train score: 0.7377022432167225\n",
      "Test score:0.6467300119842493\n",
      "\n",
      "Fold 10\n",
      " Train score: 0.7341067243796653\n",
      "Test score:0.6882549205668983\n",
      "\n",
      "Mean train score: 0.7359358819452465\n",
      " Mean test score:0.6911818826993452\n"
     ]
    }
   ],
   "source": [
    "## Cross validation\n",
    "\n",
    "\n",
    "pca = PCA(n_components = 0.8)\n",
    "lda=LinearDiscriminantAnalysis()\n",
    "data=data_uncorr.drop(columns='labels')\n",
    "labels=data_uncorr['labels']\n",
    "\n",
    "kf=KFold(n_splits=10,shuffle=True)\n",
    "folds=kf.split(data)\n",
    "train_scores=[]\n",
    "test_scores=[]\n",
    "i=0\n",
    "for train_index, test_index in folds:\n",
    "    x_train=data.values[train_index]\n",
    "    x_test=data.values[test_index]\n",
    "    y_train=labels.ravel()[train_index]\n",
    "    y_test=labels.ravel()[test_index]\n",
    "    pca.fit(x_train)\n",
    "    x_trainpca=pca.transform(x_train)\n",
    "    x_pcalda=lda.fit_transform(x_trainpca,y_train)\n",
    "    svm_mod=svm.SVC(kernel='rbf',gamma='scale',class_weight='balanced').fit(x_pcalda,y_train)\n",
    "    \n",
    "    #training scores\n",
    "    train_preds=svm_mod.predict(x_pcalda)\n",
    "    trainscore=balanced_accuracy_score(y_train,train_preds)\n",
    "    i+=1\n",
    "    print(\"Fold {}\\n Train score: {}\".format(i,trainscore))\n",
    "    train_scores.append(trainscore)\n",
    "    \n",
    "    #testing scores\n",
    "    x_testpca=pca.transform(x_test)\n",
    "    x_testpcalda=lda.transform(x_testpca)\n",
    "    test_preds=svm_mod.predict(x_testpcalda)\n",
    "    testscore=balanced_accuracy_score(y_test,test_preds)\n",
    "    print(\"Test score:{}\\n\".format(testscore))\n",
    "    test_scores.append(testscore)\n",
    "    \n",
    "print(\"Mean train score: {}\\n Mean test score:{}\".format(np.mean(train_scores),np.mean(test_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stratified kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      " Train score: 0.7373456790123457\n",
      "Test score:0.6657407407407407\n",
      "\n",
      "Fold 2\n",
      " Train score: 0.7388888888888889\n",
      "Test score:0.6157407407407408\n",
      "\n",
      "Fold 3\n",
      " Train score: 0.7353909465020575\n",
      "Test score:0.7175925925925926\n",
      "\n",
      "Fold 4\n",
      " Train score: 0.7376543209876543\n",
      "Test score:0.725\n",
      "\n",
      "Fold 5\n",
      " Train score: 0.7349794238683128\n",
      "Test score:0.6685185185185185\n",
      "\n",
      "Fold 6\n",
      " Train score: 0.7377572016460906\n",
      "Test score:0.6990740740740741\n",
      "\n",
      "Fold 7\n",
      " Train score: 0.7405349794238684\n",
      "Test score:0.6731481481481482\n",
      "\n",
      "Fold 8\n",
      " Train score: 0.7352880658436214\n",
      "Test score:0.7074074074074074\n",
      "\n",
      "Fold 9\n",
      " Train score: 0.7412551440329218\n",
      "Test score:0.6842592592592592\n",
      "\n",
      "Fold 10\n",
      " Train score: 0.7402263374485597\n",
      "Test score:0.6777777777777779\n",
      "\n",
      "Mean train score: 0.7379320987654321\n",
      " Mean test score:0.683425925925926\n"
     ]
    }
   ],
   "source": [
    "## Cross validation\n",
    "\n",
    "\n",
    "pca = PCA(n_components = 0.8)\n",
    "lda=LinearDiscriminantAnalysis()\n",
    "data=data_uncorr.drop(columns='labels')\n",
    "labels=data_uncorr['labels']\n",
    "\n",
    "kf=StratifiedKFold(n_splits=10,shuffle=True)\n",
    "folds=kf.split(data,labels)\n",
    "train_scores=[]\n",
    "test_scores=[]\n",
    "i=0\n",
    "for train_index, test_index in folds:\n",
    "    x_train=data.values[train_index]\n",
    "    x_test=data.values[test_index]\n",
    "    y_train=labels.ravel()[train_index]\n",
    "    y_test=labels.ravel()[test_index]\n",
    "    pca.fit(x_train)\n",
    "    x_trainpca=pca.transform(x_train)\n",
    "    x_pcalda=lda.fit_transform(x_trainpca,y_train)\n",
    "    svm_mod=svm.SVC(kernel='rbf',gamma='scale',class_weight='balanced').fit(x_pcalda,y_train)\n",
    "    \n",
    "    #training scores\n",
    "    train_preds=svm_mod.predict(x_pcalda)\n",
    "    trainscore=balanced_accuracy_score(y_train,train_preds)\n",
    "    i+=1\n",
    "    print(\"Fold {}\\n Train score: {}\".format(i,trainscore))\n",
    "    train_scores.append(trainscore)\n",
    "    \n",
    "    #testing scores\n",
    "    x_testpca=pca.transform(x_test)\n",
    "    x_testpcalda=lda.transform(x_testpca)\n",
    "    test_preds=svm_mod.predict(x_testpcalda)\n",
    "    testscore=balanced_accuracy_score(y_test,test_preds)\n",
    "    print(\"Test score:{}\\n\".format(testscore))\n",
    "    test_scores.append(testscore)\n",
    "    \n",
    "print(\"Mean train score: {}\\n Mean test score:{}\".format(np.mean(train_scores),np.mean(test_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      " Train score: 0.9175954610652269\n",
      "Test score:0.6748517478152309\n",
      "Fold 2\n",
      " Train score: 0.9119630892098091\n",
      "Test score:0.707204563594535\n",
      "Fold 3\n",
      " Train score: 0.908483679895309\n",
      "Test score:0.7261801119339703\n",
      "Fold 4\n",
      " Train score: 0.9086281235667498\n",
      "Test score:0.6908444169502012\n",
      "Fold 5\n",
      " Train score: 0.9111810325654774\n",
      "Test score:0.6589924160346695\n",
      "Fold 6\n",
      " Train score: 0.9104015139328485\n",
      "Test score:0.7248504167177497\n",
      "Fold 7\n",
      " Train score: 0.9069083167689292\n",
      "Test score:0.6757105418656996\n",
      "Fold 8\n",
      " Train score: 0.909371784818816\n",
      "Test score:0.7047960402799113\n",
      "Fold 9\n",
      " Train score: 0.912020922003538\n",
      "Test score:0.6685721808783381\n",
      "Fold 10\n",
      " Train score: 0.9091012197660763\n",
      "Test score:0.6964758414929616\n",
      "Mean train score:0.9105655143592781\n",
      " mean test score:0.6928478277563268\n"
     ]
    }
   ],
   "source": [
    "## Cross validation\n",
    "\n",
    "data=data_uncorr.drop(columns='labels')\n",
    "labels=data_uncorr.labels\n",
    "\n",
    "kf=KFold(n_splits=10)\n",
    "folds=kf.split(data)\n",
    "train_scores=[]\n",
    "test_scores=[]\n",
    "i=0\n",
    "\n",
    "for train_index, test_index in folds:\n",
    "    x_train=data.values[train_index]\n",
    "    x_test=data.values[test_index]\n",
    "    y_train=labels.ravel()[train_index]\n",
    "    y_test=labels.ravel()[test_index]\n",
    "    model=svm.SVC(kernel='rbf',gamma='scale',class_weight='balanced').fit(x_train,y_train)\n",
    "    \n",
    "    #training scores\n",
    "    train_preds=model.predict(x_train)\n",
    "    trainscore=balanced_accuracy_score(y_train,train_preds)\n",
    "    i+=1\n",
    "    print(\"Fold {}\\n Train score: {}\".format(i,trainscore))\n",
    "    train_scores.append(trainscore)\n",
    "    \n",
    "    \n",
    "    #testing scores\n",
    "    test_preds=model.predict(x_test)\n",
    "    testscore=balanced_accuracy_score(y_test,test_preds)\n",
    "    print(\"Test score:{}\".format(testscore))\n",
    "    test_scores.append(testscore)\n",
    "    \n",
    "print(\"Mean train score:{}\\n mean test score:{}\".format(np.mean(train_scores),np.mean(test_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stratified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      " Train score: 0.9176954732510288\n",
      "Test score:0.6749999999999999\n",
      "Fold 2\n",
      " Train score: 0.9111111111111111\n",
      "Test score:0.7148148148148148\n",
      "Fold 3\n",
      " Train score: 0.9088477366255144\n",
      "Test score:0.7175925925925926\n",
      "Fold 4\n",
      " Train score: 0.9078189300411523\n",
      "Test score:0.6898148148148149\n",
      "Fold 5\n",
      " Train score: 0.9074074074074074\n",
      "Test score:0.6592592592592593\n",
      "Fold 6\n",
      " Train score: 0.9096707818930042\n",
      "Test score:0.725925925925926\n",
      "Fold 7\n",
      " Train score: 0.9083333333333333\n",
      "Test score:0.6722222222222222\n",
      "Fold 8\n",
      " Train score: 0.9114197530864198\n",
      "Test score:0.7101851851851851\n",
      "Fold 9\n",
      " Train score: 0.910493827160494\n",
      "Test score:0.6712962962962963\n",
      "Fold 10\n",
      " Train score: 0.9092592592592593\n",
      "Test score:0.6907407407407408\n",
      "Mean train score:0.9102057613168725\n",
      " mean test score:0.6926851851851852\n"
     ]
    }
   ],
   "source": [
    "## Cross validation\n",
    "\n",
    "data=data_uncorr.drop(columns='labels')\n",
    "labels=data_uncorr.labels\n",
    "\n",
    "kf=StratifiedKFold(n_splits=10)\n",
    "folds=kf.split(data,labels)\n",
    "train_scores=[]\n",
    "test_scores=[]\n",
    "i=0\n",
    "\n",
    "for train_index, test_index in folds:\n",
    "    x_train=data.values[train_index]\n",
    "    x_test=data.values[test_index]\n",
    "    y_train=labels.ravel()[train_index]\n",
    "    y_test=labels.ravel()[test_index]\n",
    "    model=svm.SVC(kernel='rbf',gamma='scale',class_weight='balanced').fit(x_train,y_train)\n",
    "    \n",
    "    #training scores\n",
    "    train_preds=model.predict(x_train)\n",
    "    trainscore=balanced_accuracy_score(y_train,train_preds)\n",
    "    i+=1\n",
    "    print(\"Fold {}\\n Train score: {}\".format(i,trainscore))\n",
    "    train_scores.append(trainscore)\n",
    "    \n",
    "    \n",
    "    #testing scores\n",
    "    test_preds=model.predict(x_test)\n",
    "    testscore=balanced_accuracy_score(y_test,test_preds)\n",
    "    print(\"Test score:{}\".format(testscore))\n",
    "    test_scores.append(testscore)\n",
    "    \n",
    "print(\"Mean train score:{}\\n mean test score:{}\".format(np.mean(train_scores),np.mean(test_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      " Train score: 0.9216228153566934\n",
      "Test score:0.6754313948409082\n",
      "Fold 2\n",
      " Train score: 0.9178952155596222\n",
      "Test score:0.6989150090415913\n",
      "Fold 3\n",
      " Train score: 0.9172660711614956\n",
      "Test score:0.7244018863584082\n",
      "Fold 4\n",
      " Train score: 0.9144402413118907\n",
      "Test score:0.6876222805478168\n",
      "Fold 5\n",
      " Train score: 0.9154004732724309\n",
      "Test score:0.6522029784433109\n",
      "Fold 6\n",
      " Train score: 0.9132928800398837\n",
      "Test score:0.7080410415087193\n",
      "Fold 7\n",
      " Train score: 0.9149516856881473\n",
      "Test score:0.6589521936130738\n",
      "Fold 8\n",
      " Train score: 0.9130969666798338\n",
      "Test score:0.7037990620611559\n",
      "Fold 9\n",
      " Train score: 0.9147028149877139\n",
      "Test score:0.662128653855324\n",
      "Fold 10\n",
      " Train score: 0.9148609489656073\n",
      "Test score:0.6985766464093777\n",
      "Mean train score:0.915753011302332\n",
      " mean test score:0.6870071146679686\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "data=data_uncorr.drop(columns='labels')\n",
    "labels=data_uncorr.labels\n",
    "\n",
    "clf = IsolationForest(max_samples=100, random_state=42, behaviour=\"new\",contamination=.1)\n",
    "\n",
    "clf.fit(data_uncorr.drop(columns='labels'))\n",
    "y_pred_train = clf.predict(data)\n",
    "std_data_clean = data_uncorr[np.where(y_pred_train == 1, True, False)]\n",
    "#std_data_clean.shape\n",
    "\n",
    "\n",
    "\n",
    "## Cross validation\n",
    "\n",
    "data=std_data_clean.drop(columns='labels')\n",
    "labels=std_data_clean.labels\n",
    "\n",
    "kf=KFold(n_splits=10)\n",
    "folds=kf.split(data)\n",
    "train_scores=[]\n",
    "test_scores=[]\n",
    "i=0\n",
    "\n",
    "for train_index, test_index in folds:\n",
    "    x_train=data.values[train_index]\n",
    "    x_test=data.values[test_index]\n",
    "    y_train=labels.ravel()[train_index]\n",
    "    y_test=labels.ravel()[test_index]\n",
    "    model=svm.SVC(kernel='rbf',gamma='scale',class_weight='balanced').fit(x_train,y_train)\n",
    "    \n",
    "    #training scores\n",
    "    train_preds=model.predict(x_train)\n",
    "    trainscore=balanced_accuracy_score(y_train,train_preds)\n",
    "    i+=1\n",
    "    print(\"Fold {}\\n Train score: {}\".format(i,trainscore))\n",
    "    train_scores.append(trainscore)\n",
    "    \n",
    "    \n",
    "    #testing scores\n",
    "    test_preds=model.predict(x_test)\n",
    "    testscore=balanced_accuracy_score(y_test,test_preds)\n",
    "    print(\"Test score:{}\".format(testscore))\n",
    "    test_scores.append(testscore)\n",
    "    \n",
    "print(\"Mean train score:{}\\n mean test score:{}\".format(np.mean(train_scores),np.mean(test_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stratified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      " Train score: 0.9221110379935814\n",
      "Test score:0.6711780440701957\n",
      "Fold 2\n",
      " Train score: 0.9180710463457439\n",
      "Test score:0.7000521789197164\n",
      "Fold 3\n",
      " Train score: 0.9159937207412935\n",
      "Test score:0.7166000935622009\n",
      "Fold 4\n",
      " Train score: 0.9165630123197843\n",
      "Test score:0.6845334484868113\n",
      "Fold 5\n",
      " Train score: 0.9133833957875367\n",
      "Test score:0.6534420094281911\n",
      "Fold 6\n",
      " Train score: 0.9160367596785052\n",
      "Test score:0.7152470401957608\n",
      "Fold 7\n",
      " Train score: 0.9145687487835161\n",
      "Test score:0.6700852855446401\n",
      "Fold 8\n",
      " Train score: 0.9167333186392285\n",
      "Test score:0.7065937437952384\n",
      "Fold 9\n",
      " Train score: 0.9147179369244854\n",
      "Test score:0.6747351130846013\n",
      "Fold 10\n",
      " Train score: 0.914651574677953\n",
      "Test score:0.6927492283532789\n",
      "Mean train score:0.9162830551891628\n",
      " mean test score:0.6885216185440635\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "data=data_uncorr.drop(columns='labels')\n",
    "labels=data_uncorr.labels\n",
    "\n",
    "clf = IsolationForest(max_samples=100, random_state=42, behaviour=\"new\",contamination=.1)\n",
    "\n",
    "clf.fit(data_uncorr.drop(columns='labels'))\n",
    "y_pred_train = clf.predict(data)\n",
    "std_data_clean = data_uncorr[np.where(y_pred_train == 1, True, False)]\n",
    "#std_data_clean.shape\n",
    "\n",
    "\n",
    "\n",
    "## Cross validation\n",
    "\n",
    "data=std_data_clean.drop(columns='labels')\n",
    "labels=std_data_clean.labels\n",
    "\n",
    "kf=StratifiedKFold(n_splits=10)\n",
    "folds=kf.split(data,labels)\n",
    "train_scores=[]\n",
    "test_scores=[]\n",
    "i=0\n",
    "\n",
    "for train_index, test_index in folds:\n",
    "    x_train=data.values[train_index]\n",
    "    x_test=data.values[test_index]\n",
    "    y_train=labels.ravel()[train_index]\n",
    "    y_test=labels.ravel()[test_index]\n",
    "    model=svm.SVC(kernel='rbf',gamma='scale',class_weight='balanced').fit(x_train,y_train)\n",
    "    \n",
    "    #training scores\n",
    "    train_preds=model.predict(x_train)\n",
    "    trainscore=balanced_accuracy_score(y_train,train_preds)\n",
    "    i+=1\n",
    "    print(\"Fold {}\\n Train score: {}\".format(i,trainscore))\n",
    "    train_scores.append(trainscore)\n",
    "    \n",
    "    \n",
    "    #testing scores\n",
    "    test_preds=model.predict(x_test)\n",
    "    testscore=balanced_accuracy_score(y_test,test_preds)\n",
    "    print(\"Test score:{}\".format(testscore))\n",
    "    test_scores.append(testscore)\n",
    "    \n",
    "print(\"Mean train score:{}\\n mean test score:{}\".format(np.mean(train_scores),np.mean(test_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run cross validation for each value of threshold and obtain mean cross validation score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1], [0, 1], [0, 1], [0, 1]]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=[]\n",
    "for i in range(4):\n",
    "    a.append([])\n",
    "    for j in range(2):\n",
    "        a[i].append(j)\n",
    "\n",
    "a\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1\n",
      " train score:1.0\n",
      "test score:0.4712962962962963\n",
      "fold 2\n",
      " train score:1.0\n",
      "test score:0.4666666666666666\n",
      "fold 3\n",
      " train score:1.0\n",
      "test score:0.44166666666666665\n",
      "fold 4\n",
      " train score:1.0\n",
      "test score:0.4740740740740741\n",
      "fold 5\n",
      " train score:1.0\n",
      "test score:0.41944444444444445\n",
      "fold 6\n",
      " train score:1.0\n",
      "test score:0.4648148148148148\n",
      "fold 7\n",
      " train score:1.0\n",
      "test score:0.45925925925925926\n",
      "fold 8\n",
      " train score:1.0\n",
      "test score:0.42037037037037034\n",
      "fold 9\n",
      " train score:1.0\n",
      "test score:0.4583333333333333\n",
      "fold 10\n",
      " train score:1.0\n",
      "test score:0.4361111111111111\n",
      "Threshold:1e-05\n",
      " Average train score:1.0\n",
      " Average test score:0.45120370370370366\n",
      "fold 1\n",
      " train score:1.0\n",
      "test score:0.487962962962963\n",
      "fold 2\n",
      " train score:1.0\n",
      "test score:0.4490740740740741\n",
      "fold 3\n",
      " train score:1.0\n",
      "test score:0.4537037037037037\n",
      "fold 4\n",
      " train score:1.0\n",
      "test score:0.47314814814814815\n",
      "fold 5\n",
      " train score:1.0\n",
      "test score:0.41203703703703703\n",
      "fold 6\n",
      " train score:1.0\n",
      "test score:0.4583333333333333\n",
      "fold 7\n",
      " train score:1.0\n",
      "test score:0.43703703703703706\n",
      "fold 8\n",
      " train score:1.0\n",
      "test score:0.4407407407407408\n",
      "fold 9\n",
      " train score:1.0\n",
      "test score:0.4601851851851852\n",
      "fold 10\n",
      " train score:1.0\n",
      "test score:0.4398148148148149\n",
      "Threshold:5e-05\n",
      " Average train score:1.0\n",
      " Average test score:0.4512037037037038\n",
      "fold 1\n",
      " train score:1.0\n",
      "test score:0.5\n",
      "fold 2\n",
      " train score:1.0\n",
      "test score:0.46574074074074073\n",
      "fold 3\n",
      " train score:1.0\n",
      "test score:0.4527777777777778\n",
      "fold 4\n",
      " train score:1.0\n",
      "test score:0.4611111111111111\n",
      "fold 5\n",
      " train score:1.0\n",
      "test score:0.41574074074074074\n",
      "fold 6\n",
      " train score:1.0\n",
      "test score:0.46851851851851856\n",
      "fold 7\n",
      " train score:1.0\n",
      "test score:0.4694444444444444\n",
      "fold 8\n",
      " train score:1.0\n",
      "test score:0.4138888888888889\n",
      "fold 9\n",
      " train score:1.0\n",
      "test score:0.4666666666666666\n",
      "fold 10\n",
      " train score:1.0\n",
      "test score:0.4351851851851852\n",
      "Threshold:0.0001\n",
      " Average train score:1.0\n",
      " Average test score:0.4549074074074074\n",
      "fold 1\n",
      " train score:1.0\n",
      "test score:0.47129629629629627\n",
      "fold 2\n",
      " train score:1.0\n",
      "test score:0.4666666666666666\n",
      "fold 3\n",
      " train score:1.0\n",
      "test score:0.44722222222222224\n",
      "fold 4\n",
      " train score:1.0\n",
      "test score:0.47222222222222215\n",
      "fold 5\n",
      " train score:1.0\n",
      "test score:0.4009259259259259\n",
      "fold 6\n",
      " train score:1.0\n",
      "test score:0.4518518518518519\n",
      "fold 7\n",
      " train score:1.0\n",
      "test score:0.4527777777777778\n",
      "fold 8\n",
      " train score:1.0\n",
      "test score:0.4222222222222222\n",
      "fold 9\n",
      " train score:1.0\n",
      "test score:0.47500000000000003\n",
      "fold 10\n",
      " train score:0.9993827160493827\n",
      "test score:0.44629629629629636\n",
      "Threshold:0.0005\n",
      " Average train score:0.9999382716049382\n",
      " Average test score:0.45064814814814813\n",
      "fold 1\n",
      " train score:1.0\n",
      "test score:0.48425925925925933\n",
      "fold 2\n",
      " train score:1.0\n",
      "test score:0.46574074074074073\n",
      "fold 3\n",
      " train score:1.0\n",
      "test score:0.44166666666666665\n",
      "fold 4\n",
      " train score:1.0\n",
      "test score:0.47685185185185186\n",
      "fold 5\n",
      " train score:1.0\n",
      "test score:0.40925925925925927\n",
      "fold 6\n",
      " train score:1.0\n",
      "test score:0.4703703703703703\n",
      "fold 7\n",
      " train score:1.0\n",
      "test score:0.4851851851851852\n",
      "fold 8\n",
      " train score:1.0\n",
      "test score:0.4435185185185185\n",
      "fold 9\n",
      " train score:1.0\n",
      "test score:0.4611111111111111\n",
      "fold 10\n",
      " train score:1.0\n",
      "test score:0.4388888888888889\n",
      "Threshold:0.001\n",
      " Average train score:1.0\n",
      " Average test score:0.4576851851851852\n"
     ]
    }
   ],
   "source": [
    "X=data_uncorr.drop(columns='labels')\n",
    "y=data_uncorr['labels']\n",
    "\n",
    "rf=RandomForestClassifier(n_estimators=100,class_weight='balanced')\n",
    "thresholds=[0.00001,0.00005,0.0001,0.0005,0.001]\n",
    "train_scores=[]\n",
    "test_scores=[]\n",
    "\n",
    "for i in range(len(thresholds)):\n",
    "    train_scores.append([])\n",
    "    test_scores.append([])\n",
    "    fold=1\n",
    "    skf=StratifiedKFold(10)\n",
    "    folds=skf.split(X,y)\n",
    "    for train_index, test_index in folds:\n",
    "        x_train=X.values[train_index]\n",
    "        x_test=X.values[test_index]\n",
    "        y_train=y.ravel()[train_index]\n",
    "        y_test=y.ravel()[test_index]\n",
    "        rf.fit(x_train,y_train)\n",
    "        sfm=SelectFromModel(rf,prefit=True,threshold=thresholds[i])\n",
    "        #training\n",
    "        x_train_new=sfm.transform(x_train)\n",
    "        x_test_new=sfm.transform(x_test)\n",
    "        rf.fit(x_train_new,y_train)\n",
    "        train_preds=rf.predict(x_train_new)\n",
    "        scores=balanced_accuracy_score(y_train,train_preds)\n",
    "        print(\"fold {}\\n train score:{}\".format(fold,scores))\n",
    "        train_scores[i].append(scores)\n",
    "        #testing\n",
    "       \n",
    "        test_preds=rf.predict(x_test_new)\n",
    "        scores=balanced_accuracy_score(y_test,test_preds)\n",
    "        print(\"test score:{}\".format(scores))\n",
    "        test_scores[i].append(scores)\n",
    "        fold += 1\n",
    "        \n",
    "    print(\"Threshold:{}\\n Average train score:{}\\n Average test score:{}\".format(thresholds[i],np.mean(train_scores[i]),np.mean(test_scores[i])))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starts CV for threshold: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Cristy/.virtualenvs/myenv/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1\n",
      " train score:0.888477366255144\n",
      "test score:0.648148148148148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Cristy/.virtualenvs/myenv/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 2\n",
      " train score:0.886008230452675\n",
      "test score:0.6583333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Cristy/.virtualenvs/myenv/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 3\n",
      " train score:0.8887860082304527\n",
      "test score:0.6648148148148149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Cristy/.virtualenvs/myenv/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 4\n",
      " train score:0.8845679012345679\n",
      "test score:0.6694444444444444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Cristy/.virtualenvs/myenv/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 5\n",
      " train score:0.8866255144032921\n",
      "test score:0.6305555555555555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Cristy/.virtualenvs/myenv/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 6\n",
      " train score:0.8859053497942387\n",
      "test score:0.6962962962962963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Cristy/.virtualenvs/myenv/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 7\n",
      " train score:0.8855967078189301\n",
      "test score:0.6694444444444444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Cristy/.virtualenvs/myenv/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 8\n",
      " train score:0.8852880658436214\n",
      "test score:0.6638888888888889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Cristy/.virtualenvs/myenv/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 9\n",
      " train score:0.8880658436213992\n",
      "test score:0.6453703703703704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Cristy/.virtualenvs/myenv/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 10\n",
      " train score:0.8872427983539094\n",
      "test score:0.6361111111111111\n",
      "Average train score:0.886656378600823\n",
      " Average test score:0.6582407407407407\n",
      "Starts CV for threshold: 0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Cristy/.virtualenvs/myenv/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1\n",
      " train score:0.8889917695473252\n",
      "test score:0.6537037037037038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Cristy/.virtualenvs/myenv/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 2\n",
      " train score:0.886522633744856\n",
      "test score:0.6583333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Cristy/.virtualenvs/myenv/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 3\n",
      " train score:0.8875514403292181\n",
      "test score:0.6759259259259259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Cristy/.virtualenvs/myenv/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 4\n",
      " train score:0.8855967078189301\n",
      "test score:0.6666666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Cristy/.virtualenvs/myenv/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 5\n",
      " train score:0.8840534979423867\n",
      "test score:0.6361111111111111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Cristy/.virtualenvs/myenv/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 6\n",
      " train score:0.8868312757201645\n",
      "test score:0.7018518518518518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Cristy/.virtualenvs/myenv/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 7\n",
      " train score:0.8858024691358025\n",
      "test score:0.6694444444444444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Cristy/.virtualenvs/myenv/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 8\n",
      " train score:0.8856995884773662\n",
      "test score:0.6703703703703704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Cristy/.virtualenvs/myenv/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 9\n",
      " train score:0.8876543209876543\n",
      "test score:0.650925925925926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Cristy/.virtualenvs/myenv/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 10\n",
      " train score:0.8874485596707818\n",
      "test score:0.637037037037037\n",
      "Average train score:0.8866152263374486\n",
      " Average test score:0.662037037037037\n",
      "Starts CV for threshold: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Cristy/.virtualenvs/myenv/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1\n",
      " train score:0.875\n",
      "test score:0.639814814814815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Cristy/.virtualenvs/myenv/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 2\n",
      " train score:0.8777777777777778\n",
      "test score:0.6537037037037038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Cristy/.virtualenvs/myenv/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 3\n",
      " train score:0.8790123456790123\n",
      "test score:0.687962962962963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Cristy/.virtualenvs/myenv/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 4\n",
      " train score:0.8760288065843621\n",
      "test score:0.6712962962962964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Cristy/.virtualenvs/myenv/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 5\n",
      " train score:0.8665637860082306\n",
      "test score:0.6314814814814814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Cristy/.virtualenvs/myenv/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 6\n",
      " train score:0.8776748971193417\n",
      "test score:0.7046296296296296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Cristy/.virtualenvs/myenv/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 7\n",
      " train score:0.8760288065843621\n",
      "test score:0.6685185185185185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Cristy/.virtualenvs/myenv/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 8\n",
      " train score:0.8729423868312757\n",
      "test score:0.6694444444444444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Cristy/.virtualenvs/myenv/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 9\n",
      " train score:0.8783950617283951\n",
      "test score:0.6425925925925926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Cristy/.virtualenvs/myenv/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 10\n",
      " train score:0.87880658436214\n",
      "test score:0.6462962962962964\n",
      "Average train score:0.8758230452674898\n",
      " Average test score:0.6615740740740741\n",
      "Starts CV for threshold: 0.005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Cristy/.virtualenvs/myenv/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1\n",
      " train score:0.6709876543209875\n",
      "test score:0.6175925925925926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Cristy/.virtualenvs/myenv/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 2\n",
      " train score:0.7117283950617285\n",
      "test score:0.6296296296296297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Cristy/.virtualenvs/myenv/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 3\n",
      " train score:0.6758230452674897\n",
      "test score:0.6259259259259259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Cristy/.virtualenvs/myenv/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 4\n",
      " train score:0.6943415637860082\n",
      "test score:0.5898148148148148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Cristy/.virtualenvs/myenv/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 5\n",
      " train score:0.6922839506172839\n",
      "test score:0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Cristy/.virtualenvs/myenv/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 6\n",
      " train score:0.7111111111111111\n",
      "test score:0.6157407407407408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Cristy/.virtualenvs/myenv/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 7\n",
      " train score:0.7251028806584362\n",
      "test score:0.5703703703703704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Cristy/.virtualenvs/myenv/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 8\n",
      " train score:0.6930041152263374\n",
      "test score:0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Cristy/.virtualenvs/myenv/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 9\n",
      " train score:0.6915637860082304\n",
      "test score:0.6342592592592592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Cristy/.virtualenvs/myenv/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 10\n",
      " train score:0.7114197530864197\n",
      "test score:0.6064814814814815\n",
      "Average train score:0.6977366255144032\n",
      " Average test score:0.6089814814814816\n"
     ]
    }
   ],
   "source": [
    "X=data_uncorr.drop(columns='labels')\n",
    "y=data_uncorr['labels']\n",
    "\n",
    "rf=RandomForestClassifier(n_estimators=100,class_weight='balanced')\n",
    "thresholds=[0.0001,0.0005,0.001,0.005]\n",
    "train_scores=[]\n",
    "test_scores=[]\n",
    "\n",
    "for i in range(len(thresholds)):\n",
    "    train_scores.append([])\n",
    "    test_scores.append([])\n",
    "    fold=1\n",
    "    skf=StratifiedKFold(10)\n",
    "    folds=skf.split(X,y)\n",
    "    print(\"Starts CV for threshold: {}\".format(thresholds[i]))\n",
    "    for train_index, test_index in folds:\n",
    "        x_train=X.values[train_index]\n",
    "        x_test=X.values[test_index]\n",
    "        y_train=y.ravel()[train_index]\n",
    "        y_test=y.ravel()[test_index]\n",
    "        rf.fit(x_train,y_train)\n",
    "        sfm=SelectFromModel(rf,prefit=True,threshold=thresholds[i])\n",
    "        #training\n",
    "        x_train_new=sfm.transform(x_train)\n",
    "        x_test_new=sfm.transform(x_test)\n",
    "        svm_model=svm.SVC(kernel='poly',class_weight='balanced').fit(x_train_new,y_train)\n",
    "        train_preds=svm_model.predict(x_train_new)\n",
    "        scores=balanced_accuracy_score(y_train,train_preds)\n",
    "        print(\"fold {}\\n train score:{}\".format(fold,scores))\n",
    "        train_scores[i].append(scores)\n",
    "        #testing\n",
    "       \n",
    "        test_preds=svm_model.predict(x_test_new)\n",
    "        scores=balanced_accuracy_score(y_test,test_preds)\n",
    "        print(\"test score:{}\".format(scores))\n",
    "        test_scores[i].append(scores)\n",
    "        fold += 1\n",
    "        \n",
    "    print(\"Average train score:{}\\n Average test score:{}\".format(np.mean(train_scores[i]),np.mean(test_scores[i])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3840, 238)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0002248691420297557,\n",
       " 0.00022555300183440584,\n",
       " 0.0002462487785971787,\n",
       " 0.00028175359375109504,\n",
       " 0.00028230353069806516,\n",
       " 0.0003013531525751181,\n",
       " 0.0003049227993630898,\n",
       " 0.00030868873032755163,\n",
       " 0.0003153702054612312,\n",
       " 0.0003190178726193251,\n",
       " 0.0003194395734228285,\n",
       " 0.00032555750752748296,\n",
       " 0.000327412759795165,\n",
       " 0.00034574577218076286,\n",
       " 0.00034626786135482805,\n",
       " 0.00035210439387429126,\n",
       " 0.00035790071829879655,\n",
       " 0.0003595090279143396,\n",
       " 0.00036540849132170436,\n",
       " 0.0003676829947507269,\n",
       " 0.0003682924176836014,\n",
       " 0.0003696459656033503,\n",
       " 0.0003756840251150237,\n",
       " 0.0003758384718447716,\n",
       " 0.000379297143945136,\n",
       " 0.0003814124485210698,\n",
       " 0.0003833157470927467,\n",
       " 0.00038924746800894357,\n",
       " 0.00038968320714508216,\n",
       " 0.0003906230219250314,\n",
       " 0.00039109372148090683,\n",
       " 0.00039230788015995674,\n",
       " 0.00039526758703249454,\n",
       " 0.0003966478630852713,\n",
       " 0.0003968461230302337,\n",
       " 0.0003986787935722689,\n",
       " 0.00039869073504617935,\n",
       " 0.000399322612722849,\n",
       " 0.00040083168958621586,\n",
       " 0.0004011957919174287,\n",
       " 0.00040208993060416556,\n",
       " 0.0004026391061879694,\n",
       " 0.00040492358701212187,\n",
       " 0.0004055981171032253,\n",
       " 0.0004072681515545879,\n",
       " 0.00040785978661073174,\n",
       " 0.00041056037786408933,\n",
       " 0.0004114941315320173,\n",
       " 0.0004125746106108624,\n",
       " 0.00041285631249293167,\n",
       " 0.00042069816707590303,\n",
       " 0.00042187875623855104,\n",
       " 0.00042765472290334875,\n",
       " 0.0004278179561813152,\n",
       " 0.00042792898174665,\n",
       " 0.0004281802959805359,\n",
       " 0.00042909188505093476,\n",
       " 0.0004300325441814078,\n",
       " 0.0004320083171826104,\n",
       " 0.0004326155762878796,\n",
       " 0.00043440979417352254,\n",
       " 0.00043500917894038474,\n",
       " 0.00043528509550498217,\n",
       " 0.00043662140264212135,\n",
       " 0.0004390083993312036,\n",
       " 0.0004402121047518477,\n",
       " 0.0004426265406165716,\n",
       " 0.00044318083438338,\n",
       " 0.0004432096075431117,\n",
       " 0.0004448832343117741,\n",
       " 0.0004468053216198893,\n",
       " 0.00044742770078287744,\n",
       " 0.00044841011481376955,\n",
       " 0.0004515696021561458,\n",
       " 0.0004528592848059185,\n",
       " 0.0004536175076515797,\n",
       " 0.00045366565253020583,\n",
       " 0.00045590120850120245,\n",
       " 0.0004605777296374419,\n",
       " 0.00046327102799294815,\n",
       " 0.00046459570221114925,\n",
       " 0.00046687463621382825,\n",
       " 0.0004679507701990729,\n",
       " 0.00046876966232873104,\n",
       " 0.0004693154766649653,\n",
       " 0.00047256280634316953,\n",
       " 0.0004734379589834771,\n",
       " 0.0004736520163506451,\n",
       " 0.0004755598282514184,\n",
       " 0.00047646336313666537,\n",
       " 0.0004775520983373421,\n",
       " 0.0004776112258465045,\n",
       " 0.00047850615280415135,\n",
       " 0.0004791872294934067,\n",
       " 0.00048021695452723774,\n",
       " 0.00048047005616354803,\n",
       " 0.00048145773930552806,\n",
       " 0.0004816298082836886,\n",
       " 0.00048233636972736443,\n",
       " 0.0004838502453348556,\n",
       " 0.0004848510600118836,\n",
       " 0.0004853260509279271,\n",
       " 0.0004862804623544691,\n",
       " 0.000489105721597326,\n",
       " 0.0004895154078556141,\n",
       " 0.0004898738619456291,\n",
       " 0.0004917139824554862,\n",
       " 0.0004936513793861676,\n",
       " 0.0004949477306729476,\n",
       " 0.000495113646198665,\n",
       " 0.0004953042925771335,\n",
       " 0.0004953444537639241,\n",
       " 0.00049715375602433,\n",
       " 0.0004984309904501043,\n",
       " 0.0004995320854636841,\n",
       " 0.0005011165047789266,\n",
       " 0.0005021940566293467,\n",
       " 0.0005022824277095888,\n",
       " 0.0005028560993720454,\n",
       " 0.0005040566149649834,\n",
       " 0.0005045237444169198,\n",
       " 0.0005046494869863913,\n",
       " 0.0005070548076043915,\n",
       " 0.0005081718999277979,\n",
       " 0.0005101151531662918,\n",
       " 0.0005110403368296242,\n",
       " 0.0005112223865470943,\n",
       " 0.0005133206912963075,\n",
       " 0.0005141854602885754,\n",
       " 0.0005164989013345789,\n",
       " 0.0005168620945695449,\n",
       " 0.0005174571008487326,\n",
       " 0.0005177930629837362,\n",
       " 0.0005181499037756008,\n",
       " 0.0005189275177397398,\n",
       " 0.0005196139255912289,\n",
       " 0.0005202669713166505,\n",
       " 0.0005206683222028869,\n",
       " 0.0005208391847932302,\n",
       " 0.0005246574023859016,\n",
       " 0.0005246852649130683,\n",
       " 0.0005251538149151607,\n",
       " 0.00052528306937951,\n",
       " 0.000525746177460593,\n",
       " 0.0005271161609601131,\n",
       " 0.0005271512529683639,\n",
       " 0.0005275028065548961,\n",
       " 0.0005297866331148371,\n",
       " 0.0005300723556710313,\n",
       " 0.0005307988484236384,\n",
       " 0.0005308203036830999,\n",
       " 0.0005335280186360753,\n",
       " 0.0005341831307013943,\n",
       " 0.0005354144271616147,\n",
       " 0.0005362246066767964,\n",
       " 0.0005363454440317798,\n",
       " 0.0005369543736546535,\n",
       " 0.000537446175412547,\n",
       " 0.0005387184216201104,\n",
       " 0.0005390183509635254,\n",
       " 0.0005394793077331772,\n",
       " 0.0005406744091338573,\n",
       " 0.0005415999741001976,\n",
       " 0.0005416843936276689,\n",
       " 0.0005426437936133253,\n",
       " 0.0005435973326022589,\n",
       " 0.0005441041385560478,\n",
       " 0.0005443974721079499,\n",
       " 0.0005447574175774608,\n",
       " 0.0005448797824583726,\n",
       " 0.0005449528279619631,\n",
       " 0.0005459958678416424,\n",
       " 0.0005461777103236414,\n",
       " 0.0005469737449511509,\n",
       " 0.0005469818661355158,\n",
       " 0.000547029760328265,\n",
       " 0.0005471446873183549,\n",
       " 0.0005475137290444412,\n",
       " 0.0005478674570312659,\n",
       " 0.000547962229065411,\n",
       " 0.0005483107797147109,\n",
       " 0.000548605856329511,\n",
       " 0.0005497414478961603,\n",
       " 0.0005501558176108884,\n",
       " 0.0005507714628464129,\n",
       " 0.0005508865408284825,\n",
       " 0.000551711440235777,\n",
       " 0.0005519840987664256,\n",
       " 0.0005520633155454539,\n",
       " 0.0005526966000506462,\n",
       " 0.0005538525007878181,\n",
       " 0.0005570433714634233,\n",
       " 0.0005581727614762967,\n",
       " 0.0005587973838249987,\n",
       " 0.0005590657256029437,\n",
       " 0.0005593034715825399,\n",
       " 0.0005593598813139635,\n",
       " 0.0005599990778003302,\n",
       " 0.0005607502975237083,\n",
       " 0.0005610483865026523,\n",
       " 0.0005618833338106269,\n",
       " 0.0005620516602864534,\n",
       " 0.0005625200511262089,\n",
       " 0.0005628361787513247,\n",
       " 0.000565059767713127,\n",
       " 0.0005653252022865652,\n",
       " 0.0005662059421531492,\n",
       " 0.0005663317040460192,\n",
       " 0.0005664933454702405,\n",
       " 0.0005677447374181006,\n",
       " 0.0005679006822179953,\n",
       " 0.0005686886736190414,\n",
       " 0.0005689593532962823,\n",
       " 0.0005691750520642709,\n",
       " 0.0005695827393821341,\n",
       " 0.0005697166672864464,\n",
       " 0.0005706957752333627,\n",
       " 0.0005731460782117371,\n",
       " 0.0005756403750080474,\n",
       " 0.000576743630692165,\n",
       " 0.0005769462336314414,\n",
       " 0.000578597323591901,\n",
       " 0.0005793821056056554,\n",
       " 0.0005797055881630454,\n",
       " 0.0005798665859960324,\n",
       " 0.0005801084593707368,\n",
       " 0.0005808719558172961,\n",
       " 0.0005809207162552057,\n",
       " 0.0005811302333771051,\n",
       " 0.0005815444987408577,\n",
       " 0.0005817469535160819,\n",
       " 0.0005826441212440038,\n",
       " 0.0005834205503884415,\n",
       " 0.0005834767954468634,\n",
       " 0.0005836868559942733,\n",
       " 0.0005839426302622122,\n",
       " 0.0005846776574073369,\n",
       " 0.0005860875000689795,\n",
       " 0.000586247863400782,\n",
       " 0.0005863298313246633,\n",
       " 0.0005874575194023997,\n",
       " 0.0005887312875654644,\n",
       " 0.000588911149682606,\n",
       " 0.0005897787673518954,\n",
       " 0.0005899762746183384,\n",
       " 0.0005925094247266257,\n",
       " 0.0005937022611157003,\n",
       " 0.000594236325818302,\n",
       " 0.0005949945764044109,\n",
       " 0.0005953011936364018,\n",
       " 0.0005968488954086005,\n",
       " 0.0005970232343786444,\n",
       " 0.0005974189558765631,\n",
       " 0.0005978122247567165,\n",
       " 0.0005981169591283687,\n",
       " 0.0005988032544986791,\n",
       " 0.0005992179596347604,\n",
       " 0.0005998570054452233,\n",
       " 0.0006011527529519232,\n",
       " 0.0006035404403479405,\n",
       " 0.000603923814979668,\n",
       " 0.0006050387609392296,\n",
       " 0.000605244433901651,\n",
       " 0.0006057239411872351,\n",
       " 0.0006064086788095092,\n",
       " 0.0006085462189668225,\n",
       " 0.0006095370735532082,\n",
       " 0.0006098746559976899,\n",
       " 0.0006100579197598614,\n",
       " 0.000610775440356984,\n",
       " 0.0006126050673751271,\n",
       " 0.0006129022514015092,\n",
       " 0.0006134132256182608,\n",
       " 0.0006135141690375445,\n",
       " 0.0006139366259586771,\n",
       " 0.0006145160593884414,\n",
       " 0.0006152723076282284,\n",
       " 0.0006156941429651666,\n",
       " 0.0006164916308250334,\n",
       " 0.0006175212884646881,\n",
       " 0.0006177713143160437,\n",
       " 0.0006183756585417291,\n",
       " 0.0006208081758767762,\n",
       " 0.0006215135856171785,\n",
       " 0.0006217296128763597,\n",
       " 0.0006226941894849443,\n",
       " 0.0006228195338445322,\n",
       " 0.000623247971971827,\n",
       " 0.000623494238285445,\n",
       " 0.0006239805582844179,\n",
       " 0.0006243702401079964,\n",
       " 0.0006247107669135487,\n",
       " 0.0006255176438562294,\n",
       " 0.0006258728245584548,\n",
       " 0.0006262073485520885,\n",
       " 0.0006270650220652739,\n",
       " 0.0006271212462329337,\n",
       " 0.0006276232318216615,\n",
       " 0.0006278400659857934,\n",
       " 0.0006280472113804139,\n",
       " 0.0006284361291966896,\n",
       " 0.0006299005036584896,\n",
       " 0.0006307023412803035,\n",
       " 0.0006307413790420082,\n",
       " 0.0006318038851583313,\n",
       " 0.0006323431705272405,\n",
       " 0.000633001541996633,\n",
       " 0.0006335776537057857,\n",
       " 0.0006338686026830387,\n",
       " 0.0006345565687550683,\n",
       " 0.0006348564523769827,\n",
       " 0.0006348581808834564,\n",
       " 0.0006370684624800827,\n",
       " 0.0006378781974266821,\n",
       " 0.0006379181707872007,\n",
       " 0.0006383578133848652,\n",
       " 0.0006386295436945638,\n",
       " 0.0006391977159904836,\n",
       " 0.0006393720745291373,\n",
       " 0.0006394547364281126,\n",
       " 0.0006403795993436126,\n",
       " 0.00064066926264086,\n",
       " 0.0006435743191436417,\n",
       " 0.0006440951869563519,\n",
       " 0.0006474554588984982,\n",
       " 0.0006476456149705181,\n",
       " 0.0006487081197571111,\n",
       " 0.0006504071817454645,\n",
       " 0.0006507539765244899,\n",
       " 0.0006517694698955521,\n",
       " 0.0006518921656480906,\n",
       " 0.0006525631544761261,\n",
       " 0.0006531858313451287,\n",
       " 0.000654157820490028,\n",
       " 0.0006564282421284113,\n",
       " 0.0006578421494750268,\n",
       " 0.0006580898110918075,\n",
       " 0.0006590600021523003,\n",
       " 0.0006598405951337546,\n",
       " 0.0006601302416923903,\n",
       " 0.0006602232034293152,\n",
       " 0.0006605741964357403,\n",
       " 0.0006610944381112015,\n",
       " 0.0006617191065592037,\n",
       " 0.0006628345072679891,\n",
       " 0.0006635889146467375,\n",
       " 0.0006645000272871287,\n",
       " 0.0006654236760348216,\n",
       " 0.0006668875508555284,\n",
       " 0.0006674344189388792,\n",
       " 0.0006683615741215689,\n",
       " 0.000668380719829994,\n",
       " 0.0006702543264252065,\n",
       " 0.0006706976399276826,\n",
       " 0.0006713739787959906,\n",
       " 0.0006719454805465183,\n",
       " 0.0006720130279951181,\n",
       " 0.0006723903401212343,\n",
       " 0.0006725798352874728,\n",
       " 0.0006744998447510107,\n",
       " 0.0006757568517966146,\n",
       " 0.0006766828017608034,\n",
       " 0.0006773542962508206,\n",
       " 0.000677503569003389,\n",
       " 0.0006776242881772618,\n",
       " 0.0006784488096507707,\n",
       " 0.0006788853171378311,\n",
       " 0.0006802656022300381,\n",
       " 0.0006811009030115206,\n",
       " 0.0006812775253073119,\n",
       " 0.0006820947178640862,\n",
       " 0.000682142841754465,\n",
       " 0.0006825545886073333,\n",
       " 0.0006826600555867458,\n",
       " 0.0006839631136555198,\n",
       " 0.0006840255869467222,\n",
       " 0.0006841137332885009,\n",
       " 0.0006841329639880967,\n",
       " 0.0006843960422931261,\n",
       " 0.0006845422612525348,\n",
       " 0.0006848379824490802,\n",
       " 0.0006856557656923763,\n",
       " 0.0006861812492854433,\n",
       " 0.0006876340840785772,\n",
       " 0.0006878857720772201,\n",
       " 0.0006886465996132075,\n",
       " 0.0006889221176836646,\n",
       " 0.0006889341885250915,\n",
       " 0.0006892203005398067,\n",
       " 0.0006902693059708268,\n",
       " 0.0006912242741130442,\n",
       " 0.0006919015921162105,\n",
       " 0.0006922533761105527,\n",
       " 0.0006939023601980636,\n",
       " 0.0006940437858846835,\n",
       " 0.0006942925363563137,\n",
       " 0.0006966168491066185,\n",
       " 0.0006971539866372238,\n",
       " 0.0006976111410486512,\n",
       " 0.0006976409102479118,\n",
       " 0.0006977946468447265,\n",
       " 0.0006979435307077088,\n",
       " 0.0006980354286081726,\n",
       " 0.0006986264068554409,\n",
       " 0.0006987421546776509,\n",
       " 0.00069943322250852,\n",
       " 0.000699502526264483,\n",
       " 0.0006998620831845849,\n",
       " 0.0007006947883935685,\n",
       " 0.0007010746508537404,\n",
       " 0.0007027016205532035,\n",
       " 0.0007028077633836527,\n",
       " 0.0007032815332815595,\n",
       " 0.0007033180649209014,\n",
       " 0.0007035017320156632,\n",
       " 0.0007056171145195374,\n",
       " 0.0007056424834409618,\n",
       " 0.0007060803483855538,\n",
       " 0.00070609630824855,\n",
       " 0.0007061618820799227,\n",
       " 0.0007062275144504099,\n",
       " 0.0007065760673775387,\n",
       " 0.0007079597685818725,\n",
       " 0.0007087054575284818,\n",
       " 0.0007092332993739068,\n",
       " 0.0007093275562380022,\n",
       " 0.0007128202584414528,\n",
       " 0.000713207592079721,\n",
       " 0.0007133118686396162,\n",
       " 0.0007135988241827022,\n",
       " 0.0007136542510506714,\n",
       " 0.0007138305903611548,\n",
       " 0.0007153216524581432,\n",
       " 0.0007156760889099706,\n",
       " 0.0007171913838074973,\n",
       " 0.0007177218603596658,\n",
       " 0.0007179738264562663,\n",
       " 0.0007181649971036107,\n",
       " 0.0007183156669904017,\n",
       " 0.00071959157786329,\n",
       " 0.0007205698003328241,\n",
       " 0.0007211755145809135,\n",
       " 0.000721475565791802,\n",
       " 0.0007220462959517803,\n",
       " 0.0007229265406608327,\n",
       " 0.0007242473743153182,\n",
       " 0.000724680470031932,\n",
       " 0.0007255050383853868,\n",
       " 0.0007268750762540058,\n",
       " 0.0007270921600245853,\n",
       " 0.0007275650362385552,\n",
       " 0.0007278693883245342,\n",
       " 0.0007284701392802516,\n",
       " 0.0007287605083544306,\n",
       " 0.0007300007143336043,\n",
       " 0.0007302265206094141,\n",
       " 0.0007302628892234102,\n",
       " 0.0007303229896441166,\n",
       " 0.0007313553160594295,\n",
       " 0.0007314832657501526,\n",
       " 0.0007316219585635722,\n",
       " 0.0007316466334566504,\n",
       " 0.0007329092866436592,\n",
       " 0.0007342573227571999,\n",
       " 0.0007355402693495989,\n",
       " 0.0007362497868691827,\n",
       " 0.0007379093571801163,\n",
       " 0.0007387863342091614,\n",
       " 0.0007394770888737052,\n",
       " 0.0007399136386408589,\n",
       " 0.0007402993758930693,\n",
       " 0.0007417023776386068,\n",
       " 0.0007421199568092324,\n",
       " 0.0007427206555340428,\n",
       " 0.0007434602540135404,\n",
       " 0.0007435416122262736,\n",
       " 0.0007438583827237436,\n",
       " 0.0007439421748732844,\n",
       " 0.0007442146275990481,\n",
       " 0.0007448254642144944,\n",
       " 0.0007448278715273836,\n",
       " 0.0007457366613661226,\n",
       " 0.0007458023759814059,\n",
       " 0.0007458130156546328,\n",
       " 0.0007460345228965267,\n",
       " 0.0007465138980969362,\n",
       " 0.0007470249181943613,\n",
       " 0.0007473549056566852,\n",
       " 0.0007474813089179931,\n",
       " 0.0007475454708898412,\n",
       " 0.0007514343986493662,\n",
       " 0.0007523969139769031,\n",
       " 0.0007531589370467737,\n",
       " 0.0007532121023320258,\n",
       " 0.0007554494467154911,\n",
       " 0.0007556614816734444,\n",
       " 0.0007586090576195529,\n",
       " 0.0007589591907185124,\n",
       " 0.0007597506907965856,\n",
       " 0.0007622020522309439,\n",
       " 0.0007627424809682278,\n",
       " 0.0007635235194012503,\n",
       " 0.0007648327108300451,\n",
       " 0.0007670582561931427,\n",
       " 0.0007670758302896138,\n",
       " 0.0007677052914697977,\n",
       " 0.0007686132870024119,\n",
       " 0.0007686961261811456,\n",
       " 0.000770370578291089,\n",
       " 0.0007709318673401698,\n",
       " 0.0007713961903835696,\n",
       " 0.0007723775070743203,\n",
       " 0.0007724994239524456,\n",
       " 0.0007726033093051517,\n",
       " 0.0007728456882554172,\n",
       " 0.0007736686325808197,\n",
       " 0.0007744471966819745,\n",
       " 0.0007749224921387815,\n",
       " 0.000775617035288274,\n",
       " 0.0007760300933965717,\n",
       " 0.0007772773809892813,\n",
       " 0.0007801330352897049,\n",
       " 0.0007842483556907329,\n",
       " 0.0007864050174657098,\n",
       " 0.0007873290650571154,\n",
       " 0.0007884989754215837,\n",
       " 0.000790995850626066,\n",
       " 0.0007914255168772831,\n",
       " 0.0007922152343319745,\n",
       " 0.0007928071480234195,\n",
       " 0.0007933376463259001,\n",
       " 0.0007950568399217248,\n",
       " 0.000795483660742688,\n",
       " 0.0007957202340628164,\n",
       " 0.0007968316249229695,\n",
       " 0.0008002536861152055,\n",
       " 0.0008003260677268089,\n",
       " 0.0008006021159885887,\n",
       " 0.0008012878189736105,\n",
       " 0.0008013370263480925,\n",
       " 0.0008035420715739129,\n",
       " 0.0008048772389465285,\n",
       " 0.0008052343842775009,\n",
       " 0.0008070344917469458,\n",
       " 0.0008074269898827274,\n",
       " 0.0008082182293307543,\n",
       " 0.0008083063380876577,\n",
       " 0.0008107093715086808,\n",
       " 0.000810820274339085,\n",
       " 0.0008108916291092533,\n",
       " 0.0008115031771007122,\n",
       " 0.0008125262526662173,\n",
       " 0.0008128212645542122,\n",
       " 0.0008128474109732214,\n",
       " 0.0008160719779246328,\n",
       " 0.0008164795608208912,\n",
       " 0.0008178846804355641,\n",
       " 0.0008181921237260471,\n",
       " 0.0008218749591770016,\n",
       " 0.0008221775389926406,\n",
       " 0.0008232136988679986,\n",
       " 0.0008250829860886841,\n",
       " 0.0008253497496531947,\n",
       " 0.0008263218959833142,\n",
       " 0.0008282134655200413,\n",
       " 0.0008285334239453617,\n",
       " 0.000828750489645204,\n",
       " 0.0008287598676426475,\n",
       " 0.0008288702888429359,\n",
       " 0.000829201853754779,\n",
       " 0.0008313434345150893,\n",
       " 0.0008321554475452865,\n",
       " 0.0008343193599555292,\n",
       " 0.0008349044669229061,\n",
       " 0.0008351967244708507,\n",
       " 0.0008373369972433899,\n",
       " 0.0008382982756973919,\n",
       " 0.0008414270155391048,\n",
       " 0.0008414761751127195,\n",
       " 0.0008415023473028971,\n",
       " 0.000842780802846035,\n",
       " 0.0008450268741000788,\n",
       " 0.000846965730988338,\n",
       " 0.0008500552876099532,\n",
       " 0.0008505394500833029,\n",
       " 0.0008506564973945446,\n",
       " 0.0008516066940080175,\n",
       " 0.0008526250354408564,\n",
       " 0.0008540810151686976,\n",
       " 0.000855497541479187,\n",
       " 0.0008570826115595327,\n",
       " 0.0008582946694360447,\n",
       " 0.0008585709807003481,\n",
       " 0.0008648565084374624,\n",
       " 0.0008650216166707734,\n",
       " 0.0008650851905489179,\n",
       " 0.000865619968667201,\n",
       " 0.0008756211596481763,\n",
       " 0.0008760598245468617,\n",
       " 0.0008774103065582421,\n",
       " 0.0008790280445450994,\n",
       " 0.0008795872878957677,\n",
       " 0.0008816826862925841,\n",
       " 0.0008818834555247891,\n",
       " 0.0008822605966483105,\n",
       " 0.000882914317384975,\n",
       " 0.0008843047501438229,\n",
       " 0.0008851537682393114,\n",
       " 0.0008861800004084075,\n",
       " 0.000886735737957088,\n",
       " 0.000886834977887392,\n",
       " 0.0008910480430525709,\n",
       " 0.0008919020916701775,\n",
       " 0.0008919758386545536,\n",
       " 0.0008931499365750092,\n",
       " 0.000893220926237092,\n",
       " 0.0008939398802440218,\n",
       " 0.0008940199378922325,\n",
       " 0.0008947636816940025,\n",
       " 0.0008968181894042507,\n",
       " 0.000897224942205424,\n",
       " 0.0008976486555948577,\n",
       " 0.0008991257309811647,\n",
       " 0.0008995736407783144,\n",
       " 0.0009004238037406493,\n",
       " 0.0009009476403848821,\n",
       " 0.0009010592137191679,\n",
       " 0.0009015010884091748,\n",
       " 0.0009035332292586603,\n",
       " 0.0009037064231932743,\n",
       " 0.0009047758906543782,\n",
       " 0.0009074839779140261,\n",
       " 0.0009078068261000014,\n",
       " 0.0009088209178369896,\n",
       " 0.0009088467262776469,\n",
       " 0.0009108095893788906,\n",
       " 0.0009114524456082507,\n",
       " 0.0009119173500236941,\n",
       " 0.000912131475948052,\n",
       " 0.0009128344285614032,\n",
       " 0.0009147534666653754,\n",
       " 0.0009194176550175666,\n",
       " 0.0009199381247521959,\n",
       " 0.0009203650708764063,\n",
       " 0.0009223278937890324,\n",
       " 0.0009224597266788956,\n",
       " 0.0009247684624780155,\n",
       " 0.0009275554039910907,\n",
       " 0.0009278152036175415,\n",
       " 0.0009302555406223267,\n",
       " 0.0009305431632057537,\n",
       " 0.0009305673009237752,\n",
       " 0.0009306030685353872,\n",
       " 0.0009309594969164492,\n",
       " 0.0009311220880571728,\n",
       " 0.0009317526623421492,\n",
       " 0.0009373977528562448,\n",
       " 0.0009395913926241698,\n",
       " 0.0009415832997122197,\n",
       " 0.0009420843162306633,\n",
       " 0.0009420913355318424,\n",
       " 0.0009436977014400473,\n",
       " 0.0009491871704174833,\n",
       " 0.0009492371370341752,\n",
       " 0.0009503557737197747,\n",
       " 0.000953785097808052,\n",
       " 0.0009540748802334683,\n",
       " 0.0009547115331138037,\n",
       " 0.0009561867804738243,\n",
       " 0.000956454924265781,\n",
       " 0.0009572177172752227,\n",
       " 0.0009575022810835943,\n",
       " 0.0009579740871641894,\n",
       " 0.0009580688397942079,\n",
       " 0.0009582911227694168,\n",
       " 0.0009624527953329613,\n",
       " 0.0009631337085660667,\n",
       " 0.0009656831759397179,\n",
       " 0.0009676874134512697,\n",
       " 0.0009694373651998145,\n",
       " 0.0009694701462940493,\n",
       " 0.0009698033086514876,\n",
       " 0.0009704281031472138,\n",
       " 0.0009718635125215619,\n",
       " 0.0009734076238815899,\n",
       " 0.0009774829649108832,\n",
       " 0.000977580365654585,\n",
       " 0.0009777934570865407,\n",
       " 0.0009783942065405384,\n",
       " 0.0009794076055682523,\n",
       " 0.0009795776181798631,\n",
       " 0.000982628109211383,\n",
       " 0.0009843996440248332,\n",
       " 0.0009874225446340045,\n",
       " 0.000987458852744971,\n",
       " 0.0009895178268549759,\n",
       " 0.0009900174699360275,\n",
       " 0.0009904850633540025,\n",
       " 0.0009907566942343666,\n",
       " 0.000991709681626142,\n",
       " 0.00099520680827309,\n",
       " 0.0009963252875482267,\n",
       " 0.0009989919693813906,\n",
       " 0.0009995200429714648,\n",
       " 0.0009998006186828262,\n",
       " 0.0010004550883521324,\n",
       " 0.0010019616669242157,\n",
       " 0.0010037585073421286,\n",
       " 0.0010050203721651792,\n",
       " 0.0010061031640660435,\n",
       " 0.0010063438906598248,\n",
       " 0.0010085783917803295,\n",
       " 0.00100983107870151,\n",
       " 0.0010120171639418037,\n",
       " 0.0010135861760531349,\n",
       " 0.0010158644400631125,\n",
       " 0.0010162833369300047,\n",
       " 0.001018738997997769,\n",
       " 0.0010196179808314525,\n",
       " 0.0010213742511272176,\n",
       " 0.0010213811719048957,\n",
       " 0.001021499129888659,\n",
       " 0.0010309839971414194,\n",
       " 0.0010318038388011192,\n",
       " 0.0010320004778313103,\n",
       " 0.001043445310249587,\n",
       " 0.0010453703928184148,\n",
       " 0.001049783417762473,\n",
       " 0.001050808982618648,\n",
       " 0.001053975412128461,\n",
       " 0.0010570325018911073,\n",
       " 0.0010571596756612516,\n",
       " 0.0010573746023309909,\n",
       " 0.0010576855770065238,\n",
       " 0.0010579103648797187,\n",
       " 0.0010585240563064113,\n",
       " 0.0010594497764892074,\n",
       " 0.0010598071998766218,\n",
       " 0.001064199197002314,\n",
       " 0.001064886796888625,\n",
       " 0.001067487672485728,\n",
       " 0.0010682956888042057,\n",
       " 0.001069387183767224,\n",
       " 0.0010701125558862956,\n",
       " 0.0010701698270389572,\n",
       " 0.0010702854606951986,\n",
       " 0.0010705301512536581,\n",
       " 0.00107261797060456,\n",
       " 0.0010742892134191885,\n",
       " 0.0010775262866837458,\n",
       " 0.0010775471273215139,\n",
       " 0.0010796656986529843,\n",
       " 0.0010805807369979922,\n",
       " 0.001080941992007255,\n",
       " 0.0010810641412956421,\n",
       " 0.001082892508324365,\n",
       " 0.001082901866975879,\n",
       " 0.0010875600214362746,\n",
       " 0.0010880244565164973,\n",
       " 0.0010887522740803044,\n",
       " 0.001092347934124529,\n",
       " 0.001092659032028437,\n",
       " 0.0010940996868826225,\n",
       " 0.0010944188847777643,\n",
       " 0.0011023765283714407,\n",
       " 0.0011059222846082275,\n",
       " 0.0011059403197579921,\n",
       " 0.001108699586201829,\n",
       " 0.0011122603092883455,\n",
       " 0.0011141942154167322,\n",
       " 0.0011213108627312702,\n",
       " 0.00112337348672095,\n",
       " 0.001123849502781043,\n",
       " 0.0011273618612686442,\n",
       " 0.0011292072094831816,\n",
       " 0.0011292260881207675,\n",
       " 0.001132518088373742,\n",
       " 0.0011342364897534178,\n",
       " 0.0011448144699393644,\n",
       " 0.001156010861173244,\n",
       " 0.001156193630790082,\n",
       " 0.001161505554457483,\n",
       " 0.001164307386823919,\n",
       " 0.001165262785563011,\n",
       " 0.0011714906341366896,\n",
       " 0.001173965994789153,\n",
       " 0.0011775448504511134,\n",
       " 0.0011854204682930384,\n",
       " 0.0011854676387700817,\n",
       " 0.001186240843453537,\n",
       " 0.0011896434521415976,\n",
       " 0.0011936209731205055,\n",
       " 0.0011945465735025956,\n",
       " 0.0012152172412447275,\n",
       " 0.0012214175107337572,\n",
       " 0.0012326651059588632,\n",
       " 0.001232905046562516,\n",
       " 0.0012335490320554109,\n",
       " 0.0012356025379307931,\n",
       " 0.0012364711392109638,\n",
       " 0.001238398482366386,\n",
       " 0.00123963489590971,\n",
       " 0.0012473206449657406,\n",
       " 0.001250959354995484,\n",
       " 0.001253752888316145,\n",
       " 0.0012542989858850796,\n",
       " 0.0012578316091311833,\n",
       " 0.001258503275687895,\n",
       " 0.0012652519225613667,\n",
       " 0.001265491061758378,\n",
       " 0.001269206537318716,\n",
       " 0.00127012682811028,\n",
       " 0.0012720887188903238,\n",
       " 0.0012773332041882233,\n",
       " 0.0012830398205844085,\n",
       " 0.0012849705337316998,\n",
       " 0.0012855719327723302,\n",
       " 0.0012859259650316727,\n",
       " 0.001296826531843101,\n",
       " 0.0013024901482898731,\n",
       " 0.0013025686712304912,\n",
       " 0.0013154175014752712,\n",
       " 0.0013168456083806146,\n",
       " 0.00132538667848423,\n",
       " 0.001325907300939086,\n",
       " 0.001330605640450388,\n",
       " 0.0013335152660351437,\n",
       " 0.0013419348270251594,\n",
       " 0.00134235117317849,\n",
       " 0.0013438981107071605,\n",
       " 0.0013452245952845253,\n",
       " 0.0013520108495366325,\n",
       " 0.001371591602269021,\n",
       " 0.001373318893581724,\n",
       " 0.0013740927003026413,\n",
       " 0.0013780421701854104,\n",
       " 0.0013920424030724166,\n",
       " 0.0013939547289778154,\n",
       " 0.0013946156742733343,\n",
       " 0.0013983031171786097,\n",
       " 0.001403094456475272,\n",
       " 0.0014243080101683035,\n",
       " 0.00143599961613603,\n",
       " 0.0014453406620970508,\n",
       " 0.0014487447722619955,\n",
       " 0.0014488743834894873,\n",
       " 0.0014493150561655718,\n",
       " 0.0014534681171933421,\n",
       " 0.0014623660691797393,\n",
       " 0.0014643146871120157,\n",
       " 0.001467561068827166,\n",
       " 0.00146861707889786,\n",
       " 0.0014730430822529838,\n",
       " 0.0014879790590072012,\n",
       " 0.0014923978887030176,\n",
       " 0.0014975817581721151,\n",
       " 0.0015016823843589941,\n",
       " 0.0015042960505025876,\n",
       " 0.0015045863889556665,\n",
       " 0.0015116744155252299,\n",
       " 0.0015144032045303348,\n",
       " 0.0015147549669326786,\n",
       " 0.0015364157928713345,\n",
       " 0.0015405940047800003,\n",
       " 0.0015464083759014332,\n",
       " 0.001560509754007028,\n",
       " 0.0015841116181051485,\n",
       " 0.0015868845480058356,\n",
       " 0.0016081666665263103,\n",
       " 0.001615761220376364,\n",
       " 0.0016295026706223147,\n",
       " 0.0016330323048081357,\n",
       " 0.0016457000167333278,\n",
       " 0.001648107534621363,\n",
       " 0.001671982266632033,\n",
       " 0.001678086531716296,\n",
       " 0.0016951917603964548,\n",
       " 0.001714958626162098,\n",
       " 0.0017177748358048564,\n",
       " 0.0018100041085373315,\n",
       " 0.0018193765370027471,\n",
       " 0.0018215185347855531,\n",
       " 0.0018709891177888566,\n",
       " 0.0018854054666702475,\n",
       " 0.0019224136959720844,\n",
       " 0.0019377790835736106,\n",
       " 0.0019603452973568094,\n",
       " 0.002003833874041572,\n",
       " 0.0020295513404987413,\n",
       " 0.002039489892974146,\n",
       " 0.002074727551747115,\n",
       " 0.0022156937983763498,\n",
       " 0.00221967551092759,\n",
       " 0.002221336589360649,\n",
       " 0.002224017204117275,\n",
       " 0.002225882599619693,\n",
       " 0.002228287172512247,\n",
       " 0.002228848196677244,\n",
       " 0.002249612070183996,\n",
       " 0.0022550643675552278,\n",
       " 0.0022609862249378986,\n",
       " 0.0022844024544189504,\n",
       " 0.0022950922858742105,\n",
       " 0.0023281948340766008,\n",
       " 0.002379962402414208,\n",
       " 0.002381956921405319,\n",
       " 0.002467098371916476,\n",
       " 0.002517574903034067,\n",
       " 0.002546827489392653,\n",
       " 0.0025501542367326324,\n",
       " 0.0025570416791860394,\n",
       " 0.0025580499816644825,\n",
       " 0.0026319469360451464,\n",
       " 0.0026386467495722347,\n",
       " 0.0026628015427055153,\n",
       " 0.002699198511188017,\n",
       " 0.002711162508177856,\n",
       " 0.0027123589770601505,\n",
       " 0.0027240544847281726,\n",
       " 0.002734476683804439,\n",
       " 0.002763940041086947,\n",
       " 0.002821634886446055,\n",
       " 0.0028261227579543676,\n",
       " 0.0028826739342013457,\n",
       " 0.0029324900851649717,\n",
       " 0.002971634316298645,\n",
       " 0.0030571604603082374,\n",
       " 0.0030591051744685935,\n",
       " 0.0031927618211341634,\n",
       " 0.00326145234934225,\n",
       " 0.0033513947452514735,\n",
       " 0.003425839877782478,\n",
       " 0.0034305476426762754,\n",
       " 0.0034579138166583824,\n",
       " 0.0035677003991706592,\n",
       " 0.003584183043042142,\n",
       " 0.0036544099285770247,\n",
       " 0.004036084781486186,\n",
       " 0.004221809248533245,\n",
       " 0.004261742218228351,\n",
       " 0.00438755033676606,\n",
       " 0.004522311997855282,\n",
       " 0.004556992989949536,\n",
       " 0.004768926457817782,\n",
       " 0.005059708962255906,\n",
       " 0.005087654546580309,\n",
       " 0.0053672891611289365,\n",
       " 0.005457814474884582,\n",
       " 0.005662667679670163,\n",
       " 0.005890041699182712,\n",
       " 0.00627811184245341,\n",
       " 0.006402529444502695,\n",
       " 0.006408536950873847,\n",
       " 0.006601366930230892,\n",
       " 0.006946629329261548,\n",
       " 0.007493856759005299,\n",
       " 0.007661133846906916,\n",
       " 0.008103533054286294,\n",
       " 0.008132599694843638,\n",
       " 0.008566869642669771,\n",
       " 0.008678660680352003,\n",
       " 0.00919751314116962,\n",
       " 0.00987719261087383]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "\n",
    "rf_fs=RandomForestClassifier(n_estimators=100,class_weight='balanced',random_state=42).fit(X_train,y_train)\n",
    "sf=SelectFromModel(rf_fs,prefit=True)\n",
    "x_train_fs=sf.transform(X_train)\n",
    "print(x_train_fs.shape)\n",
    "rf_fs.feature_importances_\n",
    "sorted(rf_fs.feature_importances_,reverse=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 2., 3., 4.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linspace(0,4,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starts CV for threshold: 0.0001\n",
      "(3840, 963)\n",
      "fold 1\n",
      " train score:0.9181712962962963\n",
      "test score:0.6949074074074074\n",
      "(3840, 963)\n",
      "fold 2\n",
      " train score:0.9113425925925925\n",
      "test score:0.7041666666666666\n",
      "(3840, 963)\n",
      "fold 3\n",
      " train score:0.9069444444444444\n",
      "test score:0.6824074074074075\n",
      "(3840, 963)\n",
      "fold 4\n",
      " train score:0.9096064814814815\n",
      "test score:0.6912037037037037\n",
      "(3840, 963)\n",
      "fold 5\n",
      " train score:0.9096064814814815\n",
      "test score:0.6717592592592592\n",
      "Average train score:0.9111342592592593\n",
      " Average test score:0.6888888888888889\n",
      "Starts CV for threshold: 0.00019999999999999998\n",
      "(3840, 963)\n",
      "fold 1\n",
      " train score:0.9181712962962963\n",
      "test score:0.6949074074074074\n",
      "(3840, 963)\n",
      "fold 2\n",
      " train score:0.9113425925925925\n",
      "test score:0.7041666666666666\n",
      "(3840, 963)\n",
      "fold 3\n",
      " train score:0.9069444444444444\n",
      "test score:0.6824074074074075\n",
      "(3840, 963)\n",
      "fold 4\n",
      " train score:0.9096064814814815\n",
      "test score:0.6912037037037037\n",
      "(3840, 962)\n",
      "fold 5\n",
      " train score:0.9097222222222223\n",
      "test score:0.674537037037037\n",
      "Average train score:0.9111574074074074\n",
      " Average test score:0.6894444444444444\n",
      "Starts CV for threshold: 0.0003\n",
      "(3840, 960)\n",
      "fold 1\n",
      " train score:0.9187500000000001\n",
      "test score:0.700925925925926\n",
      "(3840, 958)\n",
      "fold 2\n",
      " train score:0.9118055555555555\n",
      "test score:0.7069444444444445\n",
      "(3840, 958)\n",
      "fold 3\n",
      " train score:0.9075231481481482\n",
      "test score:0.6851851851851851\n",
      "(3840, 958)\n",
      "fold 4\n",
      " train score:0.9092592592592593\n",
      "test score:0.6884259259259259\n",
      "(3840, 958)\n",
      "fold 5\n",
      " train score:0.9101851851851852\n",
      "test score:0.6791666666666667\n",
      "Average train score:0.9115046296296297\n",
      " Average test score:0.6921296296296295\n",
      "Starts CV for threshold: 0.00039999999999999996\n",
      "(3840, 934)\n",
      "fold 1\n",
      " train score:0.9170138888888889\n",
      "test score:0.6935185185185185\n",
      "(3840, 929)\n",
      "fold 2\n",
      " train score:0.9116898148148148\n",
      "test score:0.7074074074074074\n",
      "(3840, 930)\n",
      "fold 3\n",
      " train score:0.9056712962962963\n",
      "test score:0.6800925925925926\n",
      "(3840, 930)\n",
      "fold 4\n",
      " train score:0.9083333333333333\n",
      "test score:0.6893518518518519\n",
      "(3840, 928)\n",
      "fold 5\n",
      " train score:0.911574074074074\n",
      "test score:0.6819444444444445\n",
      "Average train score:0.9108564814814815\n",
      " Average test score:0.6904629629629631\n",
      "Starts CV for threshold: 0.0005\n",
      "(3840, 855)\n",
      "fold 1\n",
      " train score:0.9120370370370371\n",
      "test score:0.6962962962962963\n",
      "(3840, 839)\n",
      "fold 2\n",
      " train score:0.908449074074074\n",
      "test score:0.6976851851851852\n",
      "(3840, 847)\n",
      "fold 3\n",
      " train score:0.9074074074074074\n",
      "test score:0.6828703703703703\n",
      "(3840, 867)\n",
      "fold 4\n",
      " train score:0.9086805555555556\n",
      "test score:0.6824074074074074\n",
      "(3840, 845)\n",
      "fold 5\n",
      " train score:0.907638888888889\n",
      "test score:0.6837962962962963\n",
      "Average train score:0.9088425925925927\n",
      " Average test score:0.6886111111111111\n",
      "Starts CV for threshold: 0.0006000000000000001\n",
      "(3840, 722)\n",
      "fold 1\n",
      " train score:0.9116898148148148\n",
      "test score:0.6958333333333333\n",
      "(3840, 725)\n",
      "fold 2\n",
      " train score:0.9069444444444446\n",
      "test score:0.7138888888888889\n",
      "(3840, 705)\n",
      "fold 3\n",
      " train score:0.898726851851852\n",
      "test score:0.6995370370370372\n",
      "(3840, 718)\n",
      "fold 4\n",
      " train score:0.8980324074074074\n",
      "test score:0.6986111111111111\n",
      "(3840, 711)\n",
      "fold 5\n",
      " train score:0.9046296296296296\n",
      "test score:0.6694444444444444\n",
      "Average train score:0.9040046296296296\n",
      " Average test score:0.695462962962963\n",
      "Starts CV for threshold: 0.0007\n",
      "(3840, 568)\n",
      "fold 1\n",
      " train score:0.9024305555555555\n",
      "test score:0.700925925925926\n",
      "(3840, 578)\n",
      "fold 2\n",
      " train score:0.8962962962962964\n",
      "test score:0.7111111111111111\n",
      "(3840, 559)\n",
      "fold 3\n",
      " train score:0.8935185185185185\n",
      "test score:0.6865740740740741\n",
      "(3840, 596)\n",
      "fold 4\n",
      " train score:0.8913194444444444\n",
      "test score:0.6777777777777777\n",
      "(3840, 556)\n",
      "fold 5\n",
      " train score:0.8976851851851851\n",
      "test score:0.6842592592592593\n",
      "Average train score:0.89625\n",
      " Average test score:0.6921296296296297\n",
      "Starts CV for threshold: 0.0007999999999999999\n",
      "(3840, 443)\n",
      "fold 1\n",
      " train score:0.892013888888889\n",
      "test score:0.687037037037037\n",
      "(3840, 439)\n",
      "fold 2\n",
      " train score:0.892824074074074\n",
      "test score:0.7124999999999999\n",
      "(3840, 426)\n",
      "fold 3\n",
      " train score:0.8855324074074074\n",
      "test score:0.7032407407407407\n",
      "(3840, 441)\n",
      "fold 4\n",
      " train score:0.8828703703703704\n",
      "test score:0.6805555555555557\n",
      "(3840, 422)\n",
      "fold 5\n",
      " train score:0.8905092592592592\n",
      "test score:0.6694444444444444\n",
      "Average train score:0.8887499999999999\n",
      " Average test score:0.6905555555555556\n",
      "Starts CV for threshold: 0.0009\n",
      "(3840, 334)\n",
      "fold 1\n",
      " train score:0.8766203703703704\n",
      "test score:0.6921296296296297\n",
      "(3840, 339)\n",
      "fold 2\n",
      " train score:0.8792824074074073\n",
      "test score:0.6986111111111111\n",
      "(3840, 322)\n",
      "fold 3\n",
      " train score:0.8709490740740741\n",
      "test score:0.6939814814814814\n",
      "(3840, 336)\n",
      "fold 4\n",
      " train score:0.8707175925925926\n",
      "test score:0.6800925925925926\n",
      "(3840, 325)\n",
      "fold 5\n",
      " train score:0.8730324074074075\n",
      "test score:0.6513888888888889\n",
      "Average train score:0.8741203703703704\n",
      " Average test score:0.6832407407407407\n",
      "Starts CV for threshold: 0.001\n",
      "(3840, 256)\n",
      "fold 1\n",
      " train score:0.8752314814814816\n",
      "test score:0.6833333333333335\n",
      "(3840, 257)\n",
      "fold 2\n",
      " train score:0.8688657407407407\n",
      "test score:0.6981481481481482\n",
      "(3840, 253)\n",
      "fold 3\n",
      " train score:0.861574074074074\n",
      "test score:0.687962962962963\n",
      "(3840, 254)\n",
      "fold 4\n",
      " train score:0.8613425925925925\n",
      "test score:0.6949074074074074\n",
      "(3840, 261)\n",
      "fold 5\n",
      " train score:0.8623842592592593\n",
      "test score:0.6606481481481482\n",
      "Average train score:0.8658796296296296\n",
      " Average test score:0.685\n"
     ]
    }
   ],
   "source": [
    "X=data_uncorr.drop(columns='labels')\n",
    "y=data_uncorr['labels']\n",
    "\n",
    "rf=RandomForestClassifier(n_estimators=100,class_weight='balanced')\n",
    "thresholds=np.linspace(0.0001,0.001,10)\n",
    "train_scores=[]\n",
    "test_scores=[]\n",
    "\n",
    "for i in range(len(thresholds)):\n",
    "    train_scores.append([])\n",
    "    test_scores.append([])\n",
    "    fold=1\n",
    "    skf=StratifiedKFold(5)\n",
    "    folds=skf.split(X,y)\n",
    "    print(\"Starts CV for threshold: {}\".format(thresholds[i]))\n",
    "    for train_index, test_index in folds:\n",
    "        x_train=X.values[train_index]\n",
    "        x_test=X.values[test_index]\n",
    "        y_train=y.ravel()[train_index]\n",
    "        y_test=y.ravel()[test_index]\n",
    "        rf.fit(x_train,y_train)\n",
    "        sfm=SelectFromModel(rf,prefit=True,threshold=thresholds[i])\n",
    "        #training\n",
    "        x_train_new=sfm.transform(x_train)\n",
    "        print(x_train_new.shape)\n",
    "        x_test_new=sfm.transform(x_test)\n",
    "        svm_model=svm.SVC(kernel='rbf',class_weight='balanced',gamma='scale').fit(x_train_new,y_train)\n",
    "        train_preds=svm_model.predict(x_train_new)\n",
    "        scores=balanced_accuracy_score(y_train,train_preds)\n",
    "        print(\"fold {}\\n train score:{}\".format(fold,scores))\n",
    "        train_scores[i].append(scores)\n",
    "        #testing\n",
    "       \n",
    "        test_preds=svm_model.predict(x_test_new)\n",
    "        scores=balanced_accuracy_score(y_test,test_preds)\n",
    "        print(\"test score:{}\".format(scores))\n",
    "        test_scores[i].append(scores)\n",
    "        fold += 1\n",
    "        \n",
    "    print(\"Average train score:{}\\n Average test score:{}\".format(np.mean(train_scores[i]),np.mean(test_scores[i])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "0.0001=0.91/0.6888 (963)\n",
    "0.0002=0.91/0.6894  (963)\n",
    "0.0003=0.91/0.6921  (958)\n",
    "0.0004=0.91/0.6904  (930)\n",
    "0.0005=0.907/0.68861 (850)\n",
    "0.0006=0.904/0.6954  (720)\n",
    "0.0007=0.897/0.6921  (560)\n",
    "0.0008=0.888/0.6905 (430)\n",
    "0.0009=0.87/0.68324 (330)\n",
    "0.001=0.8623/0.685 (260)\n",
    "plt.plot(sorted(rf.feature_importances_,reverse=True))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series([2]).iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "class_weights = list(class_weight.compute_class_weight('balanced',\n",
    "                                             np.unique(data_uncorr['labels']),\n",
    "                                             data_uncorr['labels']))\n",
    "\n",
    "w_array = np.ones(labels.shape[0], dtype = 'float')\n",
    "for i, val in enumerate(labels.values):\n",
    "    w_array[i] = class_weights[pd.Series(val).iloc[0]]\n",
    "\n",
    "#data_uncorr['weights']=pd.DataFrame(w_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>...</th>\n",
       "      <th>x991</th>\n",
       "      <th>x992</th>\n",
       "      <th>x993</th>\n",
       "      <th>x994</th>\n",
       "      <th>x996</th>\n",
       "      <th>x997</th>\n",
       "      <th>x998</th>\n",
       "      <th>x999</th>\n",
       "      <th>labels</th>\n",
       "      <th>weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>-1.840326</td>\n",
       "      <td>1.597233</td>\n",
       "      <td>-1.566715</td>\n",
       "      <td>1.318379</td>\n",
       "      <td>1.224108</td>\n",
       "      <td>0.467427</td>\n",
       "      <td>-0.253549</td>\n",
       "      <td>1.039704</td>\n",
       "      <td>1.633808</td>\n",
       "      <td>4.417618</td>\n",
       "      <td>...</td>\n",
       "      <td>1.888482</td>\n",
       "      <td>-1.162274</td>\n",
       "      <td>-0.364498</td>\n",
       "      <td>3.890447</td>\n",
       "      <td>-2.940528</td>\n",
       "      <td>2.811817</td>\n",
       "      <td>-2.298899</td>\n",
       "      <td>2.524743</td>\n",
       "      <td>1</td>\n",
       "      <td>0.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.619528</td>\n",
       "      <td>1.623244</td>\n",
       "      <td>0.278015</td>\n",
       "      <td>0.514886</td>\n",
       "      <td>0.944344</td>\n",
       "      <td>0.104680</td>\n",
       "      <td>0.645978</td>\n",
       "      <td>0.923262</td>\n",
       "      <td>-0.443695</td>\n",
       "      <td>-0.067931</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.477999</td>\n",
       "      <td>0.175075</td>\n",
       "      <td>1.183244</td>\n",
       "      <td>-2.431837</td>\n",
       "      <td>0.129538</td>\n",
       "      <td>-1.352264</td>\n",
       "      <td>0.354308</td>\n",
       "      <td>0.407535</td>\n",
       "      <td>0</td>\n",
       "      <td>2.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-0.710338</td>\n",
       "      <td>-1.264436</td>\n",
       "      <td>0.194246</td>\n",
       "      <td>1.200533</td>\n",
       "      <td>1.786156</td>\n",
       "      <td>-0.306610</td>\n",
       "      <td>1.386083</td>\n",
       "      <td>0.766600</td>\n",
       "      <td>0.405999</td>\n",
       "      <td>0.296851</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.601606</td>\n",
       "      <td>0.104613</td>\n",
       "      <td>0.093185</td>\n",
       "      <td>-1.634848</td>\n",
       "      <td>1.103551</td>\n",
       "      <td>-0.305443</td>\n",
       "      <td>0.692822</td>\n",
       "      <td>-0.525232</td>\n",
       "      <td>1</td>\n",
       "      <td>0.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.385612</td>\n",
       "      <td>-1.332452</td>\n",
       "      <td>-0.861293</td>\n",
       "      <td>-0.241353</td>\n",
       "      <td>-0.711425</td>\n",
       "      <td>-1.639508</td>\n",
       "      <td>0.645802</td>\n",
       "      <td>-0.445964</td>\n",
       "      <td>0.924818</td>\n",
       "      <td>1.404429</td>\n",
       "      <td>...</td>\n",
       "      <td>1.443979</td>\n",
       "      <td>-1.002650</td>\n",
       "      <td>-0.225451</td>\n",
       "      <td>-0.989392</td>\n",
       "      <td>-0.905126</td>\n",
       "      <td>0.439333</td>\n",
       "      <td>-0.309733</td>\n",
       "      <td>1.341058</td>\n",
       "      <td>1</td>\n",
       "      <td>0.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>-0.474465</td>\n",
       "      <td>-0.996560</td>\n",
       "      <td>0.815258</td>\n",
       "      <td>-1.329039</td>\n",
       "      <td>-0.352817</td>\n",
       "      <td>-0.714929</td>\n",
       "      <td>0.945220</td>\n",
       "      <td>-0.614826</td>\n",
       "      <td>-0.872365</td>\n",
       "      <td>-0.628714</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.992644</td>\n",
       "      <td>0.281183</td>\n",
       "      <td>0.657787</td>\n",
       "      <td>-0.810138</td>\n",
       "      <td>0.668942</td>\n",
       "      <td>-0.640241</td>\n",
       "      <td>-0.020962</td>\n",
       "      <td>0.136061</td>\n",
       "      <td>1</td>\n",
       "      <td>0.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>-0.570306</td>\n",
       "      <td>0.401487</td>\n",
       "      <td>0.060972</td>\n",
       "      <td>0.564311</td>\n",
       "      <td>0.691765</td>\n",
       "      <td>0.353283</td>\n",
       "      <td>0.577241</td>\n",
       "      <td>-1.509177</td>\n",
       "      <td>-0.101308</td>\n",
       "      <td>0.523942</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.157772</td>\n",
       "      <td>-0.440683</td>\n",
       "      <td>-0.292502</td>\n",
       "      <td>0.014010</td>\n",
       "      <td>-0.458611</td>\n",
       "      <td>-0.328952</td>\n",
       "      <td>-1.226622</td>\n",
       "      <td>1.038078</td>\n",
       "      <td>1</td>\n",
       "      <td>0.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>-0.069112</td>\n",
       "      <td>-0.232231</td>\n",
       "      <td>-0.271475</td>\n",
       "      <td>0.368670</td>\n",
       "      <td>-0.796288</td>\n",
       "      <td>-0.630317</td>\n",
       "      <td>0.968845</td>\n",
       "      <td>-0.772442</td>\n",
       "      <td>-0.800089</td>\n",
       "      <td>-0.138484</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.617322</td>\n",
       "      <td>0.233952</td>\n",
       "      <td>-0.181850</td>\n",
       "      <td>-0.336739</td>\n",
       "      <td>0.258821</td>\n",
       "      <td>-0.696049</td>\n",
       "      <td>-1.063482</td>\n",
       "      <td>0.097636</td>\n",
       "      <td>2</td>\n",
       "      <td>2.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>-1.139350</td>\n",
       "      <td>-1.051169</td>\n",
       "      <td>-0.652397</td>\n",
       "      <td>0.514123</td>\n",
       "      <td>-1.116970</td>\n",
       "      <td>-1.255850</td>\n",
       "      <td>0.452114</td>\n",
       "      <td>-1.015715</td>\n",
       "      <td>0.971400</td>\n",
       "      <td>0.256993</td>\n",
       "      <td>...</td>\n",
       "      <td>0.809032</td>\n",
       "      <td>0.042813</td>\n",
       "      <td>-1.724772</td>\n",
       "      <td>-0.789810</td>\n",
       "      <td>-0.928686</td>\n",
       "      <td>0.601234</td>\n",
       "      <td>-0.462123</td>\n",
       "      <td>0.826651</td>\n",
       "      <td>1</td>\n",
       "      <td>0.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>-0.046393</td>\n",
       "      <td>1.418786</td>\n",
       "      <td>0.475726</td>\n",
       "      <td>0.111345</td>\n",
       "      <td>-0.005890</td>\n",
       "      <td>0.019823</td>\n",
       "      <td>-2.858472</td>\n",
       "      <td>0.021483</td>\n",
       "      <td>1.215114</td>\n",
       "      <td>2.885537</td>\n",
       "      <td>...</td>\n",
       "      <td>0.617019</td>\n",
       "      <td>-0.442190</td>\n",
       "      <td>-2.620165</td>\n",
       "      <td>1.720000</td>\n",
       "      <td>-0.950180</td>\n",
       "      <td>1.134706</td>\n",
       "      <td>0.343368</td>\n",
       "      <td>-2.298595</td>\n",
       "      <td>1</td>\n",
       "      <td>0.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.343145</td>\n",
       "      <td>-0.499824</td>\n",
       "      <td>1.191587</td>\n",
       "      <td>0.953776</td>\n",
       "      <td>0.723424</td>\n",
       "      <td>0.054479</td>\n",
       "      <td>1.450841</td>\n",
       "      <td>-0.009794</td>\n",
       "      <td>0.778659</td>\n",
       "      <td>-0.998707</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.273638</td>\n",
       "      <td>-0.217069</td>\n",
       "      <td>-0.544206</td>\n",
       "      <td>0.005599</td>\n",
       "      <td>0.530326</td>\n",
       "      <td>0.519250</td>\n",
       "      <td>0.431584</td>\n",
       "      <td>0.719751</td>\n",
       "      <td>1</td>\n",
       "      <td>0.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.240355</td>\n",
       "      <td>-0.823836</td>\n",
       "      <td>0.024612</td>\n",
       "      <td>1.723961</td>\n",
       "      <td>-0.443556</td>\n",
       "      <td>0.958227</td>\n",
       "      <td>1.182971</td>\n",
       "      <td>-1.023064</td>\n",
       "      <td>0.032558</td>\n",
       "      <td>0.973207</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.937735</td>\n",
       "      <td>-0.000808</td>\n",
       "      <td>0.529290</td>\n",
       "      <td>-0.018024</td>\n",
       "      <td>1.659837</td>\n",
       "      <td>0.477887</td>\n",
       "      <td>0.035061</td>\n",
       "      <td>-0.073237</td>\n",
       "      <td>0</td>\n",
       "      <td>2.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.236032</td>\n",
       "      <td>-2.586838</td>\n",
       "      <td>-0.476564</td>\n",
       "      <td>0.274618</td>\n",
       "      <td>0.340005</td>\n",
       "      <td>-1.309258</td>\n",
       "      <td>-0.497197</td>\n",
       "      <td>0.048587</td>\n",
       "      <td>0.592634</td>\n",
       "      <td>1.425210</td>\n",
       "      <td>...</td>\n",
       "      <td>0.387015</td>\n",
       "      <td>-1.098613</td>\n",
       "      <td>-0.784894</td>\n",
       "      <td>-0.610867</td>\n",
       "      <td>-1.715356</td>\n",
       "      <td>1.252905</td>\n",
       "      <td>0.027794</td>\n",
       "      <td>2.025557</td>\n",
       "      <td>1</td>\n",
       "      <td>0.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.424885</td>\n",
       "      <td>0.424988</td>\n",
       "      <td>-1.137716</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.270724</td>\n",
       "      <td>-0.129615</td>\n",
       "      <td>0.328019</td>\n",
       "      <td>0.733183</td>\n",
       "      <td>-0.713563</td>\n",
       "      <td>-0.713787</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.213589</td>\n",
       "      <td>0.765407</td>\n",
       "      <td>0.201453</td>\n",
       "      <td>1.723385</td>\n",
       "      <td>0.424855</td>\n",
       "      <td>0.199291</td>\n",
       "      <td>-1.056304</td>\n",
       "      <td>-0.415796</td>\n",
       "      <td>0</td>\n",
       "      <td>2.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>-0.264833</td>\n",
       "      <td>-0.375546</td>\n",
       "      <td>-1.916264</td>\n",
       "      <td>0.911913</td>\n",
       "      <td>-0.291424</td>\n",
       "      <td>-1.846529</td>\n",
       "      <td>0.820209</td>\n",
       "      <td>1.562369</td>\n",
       "      <td>-0.459783</td>\n",
       "      <td>0.185085</td>\n",
       "      <td>...</td>\n",
       "      <td>0.676834</td>\n",
       "      <td>-0.587292</td>\n",
       "      <td>-0.530409</td>\n",
       "      <td>-0.128598</td>\n",
       "      <td>-0.803070</td>\n",
       "      <td>-0.182645</td>\n",
       "      <td>-0.568982</td>\n",
       "      <td>1.722578</td>\n",
       "      <td>1</td>\n",
       "      <td>0.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.253590</td>\n",
       "      <td>-0.630867</td>\n",
       "      <td>-2.931953</td>\n",
       "      <td>-0.869406</td>\n",
       "      <td>-2.005467</td>\n",
       "      <td>-0.879996</td>\n",
       "      <td>1.798742</td>\n",
       "      <td>-2.252853</td>\n",
       "      <td>0.440272</td>\n",
       "      <td>0.953236</td>\n",
       "      <td>...</td>\n",
       "      <td>1.109782</td>\n",
       "      <td>-0.729497</td>\n",
       "      <td>-2.218590</td>\n",
       "      <td>0.021760</td>\n",
       "      <td>-1.258273</td>\n",
       "      <td>0.873605</td>\n",
       "      <td>0.531761</td>\n",
       "      <td>1.303616</td>\n",
       "      <td>1</td>\n",
       "      <td>0.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>-0.240371</td>\n",
       "      <td>0.146863</td>\n",
       "      <td>1.085850</td>\n",
       "      <td>0.754843</td>\n",
       "      <td>-0.152472</td>\n",
       "      <td>0.367393</td>\n",
       "      <td>-2.900216</td>\n",
       "      <td>-1.057254</td>\n",
       "      <td>-0.085319</td>\n",
       "      <td>0.233810</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.067798</td>\n",
       "      <td>1.125273</td>\n",
       "      <td>0.371334</td>\n",
       "      <td>0.705608</td>\n",
       "      <td>1.146995</td>\n",
       "      <td>-1.294252</td>\n",
       "      <td>-0.328966</td>\n",
       "      <td>0.118581</td>\n",
       "      <td>0</td>\n",
       "      <td>2.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.009802</td>\n",
       "      <td>0.324230</td>\n",
       "      <td>0.680152</td>\n",
       "      <td>-0.863283</td>\n",
       "      <td>0.897400</td>\n",
       "      <td>0.363229</td>\n",
       "      <td>0.071783</td>\n",
       "      <td>0.995218</td>\n",
       "      <td>-1.120166</td>\n",
       "      <td>-1.984160</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.128230</td>\n",
       "      <td>1.787307</td>\n",
       "      <td>0.432492</td>\n",
       "      <td>0.104416</td>\n",
       "      <td>-0.069628</td>\n",
       "      <td>-1.064798</td>\n",
       "      <td>-0.268996</td>\n",
       "      <td>-1.279211</td>\n",
       "      <td>1</td>\n",
       "      <td>0.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>-0.259310</td>\n",
       "      <td>1.116803</td>\n",
       "      <td>-0.536005</td>\n",
       "      <td>-0.213399</td>\n",
       "      <td>-0.921863</td>\n",
       "      <td>0.241997</td>\n",
       "      <td>-0.767072</td>\n",
       "      <td>0.956250</td>\n",
       "      <td>-1.111940</td>\n",
       "      <td>-0.759902</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.885546</td>\n",
       "      <td>0.961603</td>\n",
       "      <td>-0.215303</td>\n",
       "      <td>0.010554</td>\n",
       "      <td>0.605008</td>\n",
       "      <td>-1.171935</td>\n",
       "      <td>-1.664783</td>\n",
       "      <td>-0.618710</td>\n",
       "      <td>1</td>\n",
       "      <td>0.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>-0.702380</td>\n",
       "      <td>-1.327558</td>\n",
       "      <td>-0.431896</td>\n",
       "      <td>0.952426</td>\n",
       "      <td>-0.220258</td>\n",
       "      <td>-1.430702</td>\n",
       "      <td>-0.918383</td>\n",
       "      <td>-1.797178</td>\n",
       "      <td>2.115137</td>\n",
       "      <td>0.558511</td>\n",
       "      <td>...</td>\n",
       "      <td>1.598031</td>\n",
       "      <td>-0.308987</td>\n",
       "      <td>0.479895</td>\n",
       "      <td>2.493218</td>\n",
       "      <td>-3.305861</td>\n",
       "      <td>3.964553</td>\n",
       "      <td>1.586046</td>\n",
       "      <td>1.158544</td>\n",
       "      <td>1</td>\n",
       "      <td>0.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.999903</td>\n",
       "      <td>0.364712</td>\n",
       "      <td>1.882645</td>\n",
       "      <td>0.554844</td>\n",
       "      <td>-0.671716</td>\n",
       "      <td>0.697074</td>\n",
       "      <td>1.619228</td>\n",
       "      <td>1.425974</td>\n",
       "      <td>-0.235808</td>\n",
       "      <td>-1.166437</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.392824</td>\n",
       "      <td>1.297028</td>\n",
       "      <td>0.698708</td>\n",
       "      <td>0.363371</td>\n",
       "      <td>1.629676</td>\n",
       "      <td>-0.387681</td>\n",
       "      <td>0.146484</td>\n",
       "      <td>0.237096</td>\n",
       "      <td>1</td>\n",
       "      <td>0.444444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows  965 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          x0        x1        x2        x3        x4        x5        x6  \\\n",
       "0  -1.840326  1.597233 -1.566715  1.318379  1.224108  0.467427 -0.253549   \n",
       "1   0.619528  1.623244  0.278015  0.514886  0.944344  0.104680  0.645978   \n",
       "2  -0.710338 -1.264436  0.194246  1.200533  1.786156 -0.306610  1.386083   \n",
       "3   1.385612 -1.332452 -0.861293 -0.241353 -0.711425 -1.639508  0.645802   \n",
       "4  -0.474465 -0.996560  0.815258 -1.329039 -0.352817 -0.714929  0.945220   \n",
       "5  -0.570306  0.401487  0.060972  0.564311  0.691765  0.353283  0.577241   \n",
       "6  -0.069112 -0.232231 -0.271475  0.368670 -0.796288 -0.630317  0.968845   \n",
       "7  -1.139350 -1.051169 -0.652397  0.514123 -1.116970 -1.255850  0.452114   \n",
       "8  -0.046393  1.418786  0.475726  0.111345 -0.005890  0.019823 -2.858472   \n",
       "9   0.343145 -0.499824  1.191587  0.953776  0.723424  0.054479  1.450841   \n",
       "10  2.240355 -0.823836  0.024612  1.723961 -0.443556  0.958227  1.182971   \n",
       "11  0.236032 -2.586838 -0.476564  0.274618  0.340005 -1.309258 -0.497197   \n",
       "12  0.424885  0.424988 -1.137716  0.000063  0.270724 -0.129615  0.328019   \n",
       "13 -0.264833 -0.375546 -1.916264  0.911913 -0.291424 -1.846529  0.820209   \n",
       "14  1.253590 -0.630867 -2.931953 -0.869406 -2.005467 -0.879996  1.798742   \n",
       "15 -0.240371  0.146863  1.085850  0.754843 -0.152472  0.367393 -2.900216   \n",
       "16  1.009802  0.324230  0.680152 -0.863283  0.897400  0.363229  0.071783   \n",
       "17 -0.259310  1.116803 -0.536005 -0.213399 -0.921863  0.241997 -0.767072   \n",
       "18 -0.702380 -1.327558 -0.431896  0.952426 -0.220258 -1.430702 -0.918383   \n",
       "19  0.999903  0.364712  1.882645  0.554844 -0.671716  0.697074  1.619228   \n",
       "\n",
       "          x7        x8        x9  ...      x991      x992      x993      x994  \\\n",
       "0   1.039704  1.633808  4.417618  ...  1.888482 -1.162274 -0.364498  3.890447   \n",
       "1   0.923262 -0.443695 -0.067931  ... -0.477999  0.175075  1.183244 -2.431837   \n",
       "2   0.766600  0.405999  0.296851  ... -0.601606  0.104613  0.093185 -1.634848   \n",
       "3  -0.445964  0.924818  1.404429  ...  1.443979 -1.002650 -0.225451 -0.989392   \n",
       "4  -0.614826 -0.872365 -0.628714  ... -1.992644  0.281183  0.657787 -0.810138   \n",
       "5  -1.509177 -0.101308  0.523942  ... -0.157772 -0.440683 -0.292502  0.014010   \n",
       "6  -0.772442 -0.800089 -0.138484  ... -0.617322  0.233952 -0.181850 -0.336739   \n",
       "7  -1.015715  0.971400  0.256993  ...  0.809032  0.042813 -1.724772 -0.789810   \n",
       "8   0.021483  1.215114  2.885537  ...  0.617019 -0.442190 -2.620165  1.720000   \n",
       "9  -0.009794  0.778659 -0.998707  ... -0.273638 -0.217069 -0.544206  0.005599   \n",
       "10 -1.023064  0.032558  0.973207  ... -0.937735 -0.000808  0.529290 -0.018024   \n",
       "11  0.048587  0.592634  1.425210  ...  0.387015 -1.098613 -0.784894 -0.610867   \n",
       "12  0.733183 -0.713563 -0.713787  ... -1.213589  0.765407  0.201453  1.723385   \n",
       "13  1.562369 -0.459783  0.185085  ...  0.676834 -0.587292 -0.530409 -0.128598   \n",
       "14 -2.252853  0.440272  0.953236  ...  1.109782 -0.729497 -2.218590  0.021760   \n",
       "15 -1.057254 -0.085319  0.233810  ... -0.067798  1.125273  0.371334  0.705608   \n",
       "16  0.995218 -1.120166 -1.984160  ... -1.128230  1.787307  0.432492  0.104416   \n",
       "17  0.956250 -1.111940 -0.759902  ... -0.885546  0.961603 -0.215303  0.010554   \n",
       "18 -1.797178  2.115137  0.558511  ...  1.598031 -0.308987  0.479895  2.493218   \n",
       "19  1.425974 -0.235808 -1.166437  ... -1.392824  1.297028  0.698708  0.363371   \n",
       "\n",
       "        x996      x997      x998      x999  labels   weights  \n",
       "0  -2.940528  2.811817 -2.298899  2.524743       1  0.444444  \n",
       "1   0.129538 -1.352264  0.354308  0.407535       0  2.666667  \n",
       "2   1.103551 -0.305443  0.692822 -0.525232       1  0.444444  \n",
       "3  -0.905126  0.439333 -0.309733  1.341058       1  0.444444  \n",
       "4   0.668942 -0.640241 -0.020962  0.136061       1  0.444444  \n",
       "5  -0.458611 -0.328952 -1.226622  1.038078       1  0.444444  \n",
       "6   0.258821 -0.696049 -1.063482  0.097636       2  2.666667  \n",
       "7  -0.928686  0.601234 -0.462123  0.826651       1  0.444444  \n",
       "8  -0.950180  1.134706  0.343368 -2.298595       1  0.444444  \n",
       "9   0.530326  0.519250  0.431584  0.719751       1  0.444444  \n",
       "10  1.659837  0.477887  0.035061 -0.073237       0  2.666667  \n",
       "11 -1.715356  1.252905  0.027794  2.025557       1  0.444444  \n",
       "12  0.424855  0.199291 -1.056304 -0.415796       0  2.666667  \n",
       "13 -0.803070 -0.182645 -0.568982  1.722578       1  0.444444  \n",
       "14 -1.258273  0.873605  0.531761  1.303616       1  0.444444  \n",
       "15  1.146995 -1.294252 -0.328966  0.118581       0  2.666667  \n",
       "16 -0.069628 -1.064798 -0.268996 -1.279211       1  0.444444  \n",
       "17  0.605008 -1.171935 -1.664783 -0.618710       1  0.444444  \n",
       "18 -3.305861  3.964553  1.586046  1.158544       1  0.444444  \n",
       "19  1.629676 -0.387681  0.146484  0.237096       1  0.444444  \n",
       "\n",
       "[20 rows x 965 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_uncorr.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   y\n",
       "0  1\n",
       "1  0\n",
       "2  1\n",
       "3  1\n",
       "4  1"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4320, 469)\n",
      "fold 6\n",
      " TRAIN SCORES:\n",
      " SVM:0.9611111111111111\n",
      " XGB:1.0\n",
      "TEST SCORES:\n",
      " SVM:0.8277777777777778\n",
      " XGB:0.8277777777777778\n",
      "(4320, 447)\n",
      "fold 6\n",
      " TRAIN SCORES:\n",
      " SVM:0.9598765432098765\n",
      " XGB:1.0\n",
      "TEST SCORES:\n",
      " SVM:0.8388888888888889\n",
      " XGB:0.8222222222222223\n",
      "(4320, 479)\n",
      "fold 6\n",
      " TRAIN SCORES:\n",
      " SVM:0.954320987654321\n",
      " XGB:1.0\n",
      "TEST SCORES:\n",
      " SVM:0.8666666666666666\n",
      " XGB:0.8444444444444444\n",
      "(4320, 457)\n",
      "fold 6\n",
      " TRAIN SCORES:\n",
      " SVM:0.9623456790123456\n",
      " XGB:1.0\n",
      "TEST SCORES:\n",
      " SVM:0.7999999999999999\n",
      " XGB:0.8166666666666668\n",
      "(4320, 457)\n",
      "fold 6\n",
      " TRAIN SCORES:\n",
      " SVM:0.9549382716049383\n",
      " XGB:1.0\n",
      "TEST SCORES:\n",
      " SVM:0.8166666666666668\n",
      " XGB:0.8166666666666668\n",
      "(4320, 450)\n",
      "fold 6\n",
      " TRAIN SCORES:\n",
      " SVM:0.9561728395061729\n",
      " XGB:1.0\n",
      "TEST SCORES:\n",
      " SVM:0.85\n",
      " XGB:0.8555555555555556\n",
      "(4320, 464)\n",
      "fold 6\n",
      " TRAIN SCORES:\n",
      " SVM:0.9629629629629629\n",
      " XGB:1.0\n",
      "TEST SCORES:\n",
      " SVM:0.8055555555555555\n",
      " XGB:0.8277777777777778\n",
      "(4320, 468)\n",
      "fold 6\n",
      " TRAIN SCORES:\n",
      " SVM:0.9635802469135802\n",
      " XGB:1.0\n",
      "TEST SCORES:\n",
      " SVM:0.8166666666666668\n",
      " XGB:0.7999999999999999\n",
      "(4320, 464)\n",
      "fold 6\n",
      " TRAIN SCORES:\n",
      " SVM:0.9580246913580247\n",
      " XGB:1.0\n",
      "TEST SCORES:\n",
      " SVM:0.7833333333333333\n",
      " XGB:0.7888888888888888\n",
      "(4320, 448)\n",
      "fold 6\n",
      " TRAIN SCORES:\n",
      " SVM:0.9586419753086419\n",
      " XGB:1.0\n",
      "TEST SCORES:\n",
      " SVM:0.7944444444444444\n",
      " XGB:0.7833333333333333\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X=data_uncorr.drop(columns='labels')\n",
    "y=data_uncorr.labels\n",
    "\n",
    "skf=StratifiedKFold(10)\n",
    "folds=skf.split(X,y)\n",
    "test_scores1=[]\n",
    "test_scores2=[]\n",
    "train_scores1=[]\n",
    "train_scores2=[]\n",
    "\n",
    "for train_index, test_index in folds:\n",
    "        x_train=X.values[train_index]\n",
    "        x_test=X.values[test_index]\n",
    "        y_train=y.ravel()[train_index]\n",
    "        y_test=y.ravel()[test_index]\n",
    "        rf.fit(x_train,y_train)\n",
    "        sfm=SelectFromModel(rf,prefit=True,threshold=0.0007)\n",
    "        #training\n",
    "        x_train_new=sfm.transform(x_train)\n",
    "        print(x_train_new.shape)\n",
    "        x_test_new=sfm.transform(x_test)\n",
    "        svm_model=svm.SVC(kernel='rbf',class_weight='balanced',gamma='scale').fit(x_train_new,y_train)\n",
    "        xgb_model = xgb.XGBClassifier(n_estimators=100, max_depth=8, learning_rate=0.1, sample_weight=w_array).fit(x_train_new,y_train)\n",
    "        train_preds1=svm_model.predict(x_train_new)\n",
    "        train_preds2=xgb_model.predict(x_train_new)\n",
    "        scores1=balanced_accuracy_score(y_train,train_preds1)\n",
    "        scores2=balanced_accuracy_score(y_train,train_preds2)\n",
    "        print(\"fold {}\\n TRAIN SCORES:\\n SVM:{}\\n XGB:{}\".format(fold,scores1,scores2))\n",
    "        train_scores1.append(scores1)\n",
    "        train_scores2.append(scores2)\n",
    "        \n",
    "        #testing\n",
    "       \n",
    "        test_preds1=svm_model.predict(x_test_new)\n",
    "        test_preds2=xgb_model.predict(x_test_new)\n",
    "        scores1=balanced_accuracy_score(y_test,test_preds1)\n",
    "        scores2=balanced_accuracy_score(y_test,test_preds2)\n",
    "        print(\"TEST SCORES:\\n SVM:{}\\n XGB:{}\".format(scores1,scores2))\n",
    "        test_scores1.append(scores1)\n",
    "        test_scores2.append(scores2)\n",
    "       \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "pipe=Pipeline([('rf', rf=RandomForestClassifier(n_estimators=100,class_weight='balanced')), \n",
    "                         ('sfm',SelectFromModel(rf,prefit=True,threshold=0.0007)), \n",
    "                         ('SVM', svm.SVC(kernel='rbf',class_weight='balanced',gamma='scale'))])\\\n",
    "\n",
    "\n",
    "\n",
    "X=data_uncorr.drop(columns='labels')\n",
    "y=data_uncorr.labels\n",
    "\n",
    "skf=StratifiedKFold(10)\n",
    "folds=skf.split(X,y)\n",
    "test_scores1=[]\n",
    "test_scores2=[]\n",
    "train_scores1=[]\n",
    "train_scores2=[]\n",
    "\n",
    "for train_index, test_index in folds:\n",
    "        x_train=X.values[train_index]\n",
    "        x_test=X.values[test_index]\n",
    "        y_train=y.ravel()[train_index]\n",
    "        y_test=y.ravel()[test_index]\n",
    "        pipe.fit(x_train,y_train)\n",
    "        train_preds=pipe.predict(x_train)\n",
    "        train_score=balanced_accuracy_score*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "estimator should be an estimator implementing 'fit' method, [('sfm', SelectFromModel(estimator=RandomForestClassifier(bootstrap=True,\n                                                 class_weight='balanced',\n                                                 criterion='gini',\n                                                 max_depth=None,\n                                                 max_features='auto',\n                                                 max_leaf_nodes=None,\n                                                 min_impurity_decrease=0.0,\n                                                 min_impurity_split=None,\n                                                 min_samples_leaf=1,\n                                                 min_samples_split=2,\n                                                 min_weight_fraction_leaf=0.0,\n                                                 n_estimators=100, n_jobs=None,\n                                                 oob_score=False,\n                                                 random_state=None, verbose=0,\n                                                 warm_start=False),\n                max_features=None, norm_order=1, prefit=False,\n                threshold=0.0007)), ('SVM', SVC(C=1.0, cache_size=200, class_weight='balanced', coef0=0.0,\n    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n    max_iter=-1, probability=False, random_state=None, shrinking=True,\n    tol=0.001, verbose=False))] was passed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-d004da89e6f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m ('SVM', svm.SVC(kernel='rbf',class_weight='balanced',gamma='scale'))]\n\u001b[1;32m     10\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipe_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mcross_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipe_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'balanced_accuracy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcross_score\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/myenv/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \"\"\"\n\u001b[1;32m    383\u001b[0m     \u001b[0;31m# To ensure multimetric format is not supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m     \u001b[0mscorer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m     cv_results = cross_validate(estimator=estimator, X=X, y=y, groups=groups,\n",
      "\u001b[0;32m~/.virtualenvs/myenv/lib/python3.7/site-packages/sklearn/metrics/scorer.py\u001b[0m in \u001b[0;36mcheck_scoring\u001b[0;34m(estimator, scoring, allow_none)\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         raise TypeError(\"estimator should be an estimator implementing \"\n\u001b[0;32m--> 270\u001b[0;31m                         \"'fit' method, %r was passed\" % estimator)\n\u001b[0m\u001b[1;32m    271\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mget_scorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: estimator should be an estimator implementing 'fit' method, [('sfm', SelectFromModel(estimator=RandomForestClassifier(bootstrap=True,\n                                                 class_weight='balanced',\n                                                 criterion='gini',\n                                                 max_depth=None,\n                                                 max_features='auto',\n                                                 max_leaf_nodes=None,\n                                                 min_impurity_decrease=0.0,\n                                                 min_impurity_split=None,\n                                                 min_samples_leaf=1,\n                                                 min_samples_split=2,\n                                                 min_weight_fraction_leaf=0.0,\n                                                 n_estimators=100, n_jobs=None,\n                                                 oob_score=False,\n                                                 random_state=None, verbose=0,\n                                                 warm_start=False),\n                max_features=None, norm_order=1, prefit=False,\n                threshold=0.0007)), ('SVM', SVC(C=1.0, cache_size=200, class_weight='balanced', coef0=0.0,\n    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n    max_iter=-1, probability=False, random_state=None, shrinking=True,\n    tol=0.001, verbose=False))] was passed"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "X=data_uncorr.drop(columns='labels')\n",
    "y=data_uncorr.labels\n",
    "\n",
    "\n",
    "pipe_steps = [('sfm',SelectFromModel(RandomForestClassifier(n_estimators=100,class_weight='balanced'),threshold=0.0007)),('SVM', svm.SVC(kernel='rbf',class_weight='balanced',gamma='scale'))]\n",
    "\n",
    "\n",
    "\n",
    "pipe_steps = [('sfm',SelectFromModel(RandomForestClassifier(n_estimators=100,class_weight='balanced'),threshold=0.0007)), \n",
    "('SVM', svm.SVC(kernel='rbf',class_weight='balanced',gamma='scale'))]\n",
    "pipeline=Pipeline(pipe_steps)\n",
    "cross_score = cross_val_score(pipe_steps, X, y, cv=10, scoring='balanced_accuracy', n_jobs=-1, verbose=2)\n",
    "print(cross_score.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6925\n",
      "[0.68518519 0.71481481 0.72222222 0.67407407 0.68796296 0.71018519\n",
      " 0.66203704 0.70648148 0.68148148 0.68055556]\n"
     ]
    }
   ],
   "source": [
    "pipe_steps = [('sfm',SelectFromModel(RandomForestClassifier(n_estimators=100,class_weight='balanced'),threshold=0.0007)), \n",
    "('SVM', svm.SVC(kernel='rbf',class_weight='balanced',gamma='scale'))]\n",
    "\n",
    "pipeline = Pipeline(pipe_steps)\n",
    "\n",
    "cv_scores = cross_val_score(pipeline, X, y, cv=10, scoring='balanced_accuracy',n_jobs =-1)\n",
    "print(cv_scores.mean())\n",
    "print(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6901851851851852\n",
      "[0.70555556 0.71203704 0.71388889 0.675      0.675      0.70277778\n",
      " 0.65185185 0.70648148 0.66944444 0.68981481]\n"
     ]
    }
   ],
   "source": [
    "pipe_steps = [('sfm',SelectFromModel(RandomForestClassifier(n_estimators=100,class_weight='balanced'),threshold=0.0007)), \n",
    "('pca',PCA(n_components = 0.8)),('SVM', svm.SVC(kernel='rbf',class_weight='balanced',gamma='scale'))]\n",
    "\n",
    "pipeline = Pipeline(pipe_steps)\n",
    "\n",
    "cv_scores = cross_val_score(pipeline, X, y, cv=10, scoring='balanced_accuracy',n_jobs =-1)\n",
    "print(cv_scores.mean())\n",
    "print(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>...</th>\n",
       "      <th>x990</th>\n",
       "      <th>x991</th>\n",
       "      <th>x992</th>\n",
       "      <th>x993</th>\n",
       "      <th>x994</th>\n",
       "      <th>x996</th>\n",
       "      <th>x997</th>\n",
       "      <th>x998</th>\n",
       "      <th>x999</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>-1.840326</td>\n",
       "      <td>1.597233</td>\n",
       "      <td>-1.566715</td>\n",
       "      <td>1.318379</td>\n",
       "      <td>1.224108</td>\n",
       "      <td>0.467427</td>\n",
       "      <td>-0.253549</td>\n",
       "      <td>1.039704</td>\n",
       "      <td>1.633808</td>\n",
       "      <td>4.417618</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.425812</td>\n",
       "      <td>1.888482</td>\n",
       "      <td>-1.162274</td>\n",
       "      <td>-0.364498</td>\n",
       "      <td>3.890447</td>\n",
       "      <td>-2.940528</td>\n",
       "      <td>2.811817</td>\n",
       "      <td>-2.298899</td>\n",
       "      <td>2.524743</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.619528</td>\n",
       "      <td>1.623244</td>\n",
       "      <td>0.278015</td>\n",
       "      <td>0.514886</td>\n",
       "      <td>0.944344</td>\n",
       "      <td>0.104680</td>\n",
       "      <td>0.645978</td>\n",
       "      <td>0.923262</td>\n",
       "      <td>-0.443695</td>\n",
       "      <td>-0.067931</td>\n",
       "      <td>...</td>\n",
       "      <td>0.292772</td>\n",
       "      <td>-0.477999</td>\n",
       "      <td>0.175075</td>\n",
       "      <td>1.183244</td>\n",
       "      <td>-2.431837</td>\n",
       "      <td>0.129538</td>\n",
       "      <td>-1.352264</td>\n",
       "      <td>0.354308</td>\n",
       "      <td>0.407535</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-0.710338</td>\n",
       "      <td>-1.264436</td>\n",
       "      <td>0.194246</td>\n",
       "      <td>1.200533</td>\n",
       "      <td>1.786156</td>\n",
       "      <td>-0.306610</td>\n",
       "      <td>1.386083</td>\n",
       "      <td>0.766600</td>\n",
       "      <td>0.405999</td>\n",
       "      <td>0.296851</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.294768</td>\n",
       "      <td>-0.601606</td>\n",
       "      <td>0.104613</td>\n",
       "      <td>0.093185</td>\n",
       "      <td>-1.634848</td>\n",
       "      <td>1.103551</td>\n",
       "      <td>-0.305443</td>\n",
       "      <td>0.692822</td>\n",
       "      <td>-0.525232</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.385612</td>\n",
       "      <td>-1.332452</td>\n",
       "      <td>-0.861293</td>\n",
       "      <td>-0.241353</td>\n",
       "      <td>-0.711425</td>\n",
       "      <td>-1.639508</td>\n",
       "      <td>0.645802</td>\n",
       "      <td>-0.445964</td>\n",
       "      <td>0.924818</td>\n",
       "      <td>1.404429</td>\n",
       "      <td>...</td>\n",
       "      <td>1.198557</td>\n",
       "      <td>1.443979</td>\n",
       "      <td>-1.002650</td>\n",
       "      <td>-0.225451</td>\n",
       "      <td>-0.989392</td>\n",
       "      <td>-0.905126</td>\n",
       "      <td>0.439333</td>\n",
       "      <td>-0.309733</td>\n",
       "      <td>1.341058</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>-0.474465</td>\n",
       "      <td>-0.996560</td>\n",
       "      <td>0.815258</td>\n",
       "      <td>-1.329039</td>\n",
       "      <td>-0.352817</td>\n",
       "      <td>-0.714929</td>\n",
       "      <td>0.945220</td>\n",
       "      <td>-0.614826</td>\n",
       "      <td>-0.872365</td>\n",
       "      <td>-0.628714</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.239761</td>\n",
       "      <td>-1.992644</td>\n",
       "      <td>0.281183</td>\n",
       "      <td>0.657787</td>\n",
       "      <td>-0.810138</td>\n",
       "      <td>0.668942</td>\n",
       "      <td>-0.640241</td>\n",
       "      <td>-0.020962</td>\n",
       "      <td>0.136061</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  964 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         x0        x1        x2        x3        x4        x5        x6  \\\n",
       "0 -1.840326  1.597233 -1.566715  1.318379  1.224108  0.467427 -0.253549   \n",
       "1  0.619528  1.623244  0.278015  0.514886  0.944344  0.104680  0.645978   \n",
       "2 -0.710338 -1.264436  0.194246  1.200533  1.786156 -0.306610  1.386083   \n",
       "3  1.385612 -1.332452 -0.861293 -0.241353 -0.711425 -1.639508  0.645802   \n",
       "4 -0.474465 -0.996560  0.815258 -1.329039 -0.352817 -0.714929  0.945220   \n",
       "\n",
       "         x7        x8        x9  ...      x990      x991      x992      x993  \\\n",
       "0  1.039704  1.633808  4.417618  ... -0.425812  1.888482 -1.162274 -0.364498   \n",
       "1  0.923262 -0.443695 -0.067931  ...  0.292772 -0.477999  0.175075  1.183244   \n",
       "2  0.766600  0.405999  0.296851  ... -0.294768 -0.601606  0.104613  0.093185   \n",
       "3 -0.445964  0.924818  1.404429  ...  1.198557  1.443979 -1.002650 -0.225451   \n",
       "4 -0.614826 -0.872365 -0.628714  ... -0.239761 -1.992644  0.281183  0.657787   \n",
       "\n",
       "       x994      x996      x997      x998      x999  labels  \n",
       "0  3.890447 -2.940528  2.811817 -2.298899  2.524743       1  \n",
       "1 -2.431837  0.129538 -1.352264  0.354308  0.407535       0  \n",
       "2 -1.634848  1.103551 -0.305443  0.692822 -0.525232       1  \n",
       "3 -0.989392 -0.905126  0.439333 -0.309733  1.341058       1  \n",
       "4 -0.810138  0.668942 -0.640241 -0.020962  0.136061       1  \n",
       "\n",
       "[5 rows x 964 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data_uncorr=data_uncorr.drop(columns='weights')]\n",
    "data_uncorr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=pd.read_csv(\"/Users/Cristy/Downloads/task2/X_test.csv\")\n",
    "test_data.drop(columns='id',inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "std_test_data=test_data.apply(lambda x: (x-x.mean())/x.std())\n",
    "test_data_uncorr=std_test_data.drop(columns=corr_feats)\n",
    "\n",
    "\n",
    "X=data_uncorr.drop(columns='labels')\n",
    "y=data_uncorr.labels\n",
    "\n",
    "\n",
    "rf=RandomForestClassifier(n_estimators=100,class_weight='balanced')\n",
    "rf.fit(X,y)\n",
    "sfm=SelectFromModel(rf,prefit=True,threshold=0.0007)\n",
    "X_train_new=sfm.transform(X)\n",
    "X_test_new=sfm.transform(test_data_uncorr)\n",
    "xgb_model = xgb.XGBClassifier(n_estimators=100, max_depth=8, learning_rate=0.1, sample_weight=w_array).fit(X_train_new,y)\n",
    "        \n",
    "predictions=xgb_model.predict(X_test_new)\n",
    "\n",
    "\n",
    "out = pd.DataFrame({'id':[float(i) for i in range(0,len(X_test_new))],'y':predictions})\n",
    "out.to_csv(\"RFFS_XGB.csv\",index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "std_test_data=test_data.apply(lambda x: (x-x.mean())/x.std())\n",
    "test_data_uncorr=std_test_data.drop(columns=corr_feats)\n",
    "\n",
    "\n",
    "X=data_uncorr.drop(columns='labels')\n",
    "y=data_uncorr.labels\n",
    "\n",
    "\n",
    "rf=RandomForestClassifier(n_estimators=100,class_weight='balanced')\n",
    "rf.fit(X,y)\n",
    "sfm=SelectFromModel(rf,prefit=True,threshold=0.0007)\n",
    "X_train_new=sfm.transform(X)\n",
    "X_test_new=sfm.transform(test_data_uncorr)\n",
    "\n",
    "\n",
    "pca = PCA(n_components = 0.8)\n",
    "pca.fit(X_train_new)\n",
    "train_data_pca = pd.DataFrame(pca.transform(X_train_new))\n",
    "test_data_pca = pd.DataFrame(pca.transform(X_test_new))\n",
    "\n",
    "\n",
    "svm_model=svm.SVC(kernel='rbf',class_weight='balanced',gamma='scale').fit(train_data_pca,y)\n",
    "        \n",
    "\n",
    "        \n",
    "predictions=svm_model.predict(test_data_pca)\n",
    "\n",
    "\n",
    "out = pd.DataFrame({'id':[float(i) for i in range(0,len(X_test_new))],'y':predictions})\n",
    "out.to_csv(\"RFFS_pcaSVM.csv\",index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "std_test_data=test_data.apply(lambda x: (x-x.mean())/x.std())\n",
    "test_data_uncorr=std_test_data.drop(columns=corr_feats)\n",
    "\n",
    "\n",
    "X=data_uncorr.drop(columns='labels')\n",
    "y=data_uncorr.labels\n",
    "\n",
    "pipeline.fit(X,y)\n",
    "predictions=pipeline.predict(test_data_uncorr)\n",
    "\n",
    "\n",
    "out = pd.DataFrame({'id':[float(i) for i in range(0,len(X_test_new))],'y':predictions})\n",
    "out.to_csv(\"pipeline_RFFS_SVM.csv\",index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
